{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cPickle as pickle\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "plt.rcParams['figure.titlesize'] = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_file = \"../../my_features/US8K_logmel128_patchslice.pickle\"\n",
    "with open(pickle_file, \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "    X_train_all = dataset[\"X_train\"]/10\n",
    "    Y_train_all = dataset[\"Y_train\"]\n",
    "    X_valid_all = dataset[\"X_valid\"]/10\n",
    "    Y_valid_all = dataset[\"Y_valid\"]\n",
    "    X_test1_all = dataset[\"X_test1\"]/10\n",
    "    Y_test1_all = dataset[\"Y_test1\"]\n",
    "    X_test2_all = dataset[\"X_test2\"]/10\n",
    "    Y_test2_all = dataset[\"Y_test2\"]\n",
    "    del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2793.   765.  2763.  2027.  2321.  2704.   213.  2625.  2717.  2800.]\n",
      "21728.0\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([2793,  765, 2763, 2027, 2321, 2704,  213, 2625, 2717, 2800]))\n"
     ]
    }
   ],
   "source": [
    "print np.sum(Y_train_all, axis = 0)\n",
    "print np.sum(Y_train_all)\n",
    "print np.unique(np.argmax(Y_train_all, axis=1), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('training data: ', (21728, 16384), (21728, 10))\n",
      "[ 2793.   765.  2763.  2027.  2321.  2704.   213.  2625.  2717.  2800.]\n"
     ]
    }
   ],
   "source": [
    "print ('training data: ' , X_train_all.shape, Y_train_all.shape)\n",
    "\n",
    "print (np.sum(Y_train_all, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Use 2 classes as training data\n",
    "# X_train = X_train_all[(np.argmax(Y_train_all, axis=1) == 4)+(np.argmax(Y_train_all, axis=1) == 0)]\n",
    "# Y_train = Y_train_all[(np.argmax(Y_train_all, axis=1) == 4)+(np.argmax(Y_train_all, axis=1) == 0)][:,(0,4)]\n",
    "\n",
    "# X_valid = X_valid_all[(np.argmax(Y_valid_all, axis=1) == 4)+(np.argmax(Y_valid_all, axis=1) == 0)]\n",
    "# Y_valid = Y_valid_all[(np.argmax(Y_valid_all, axis=1) == 4)+(np.argmax(Y_valid_all, axis=1) == 0)][:,(0,4)]\n",
    "\n",
    "# X_test1 = X_test1_all[(np.argmax(Y_test1_all, axis=1) == 4)+(np.argmax(Y_test1_all, axis=1) == 0)]\n",
    "# Y_test1 = Y_test1_all[(np.argmax(Y_test1_all, axis=1) == 4)+(np.argmax(Y_test1_all, axis=1) == 0)][:,(0,4)]\n",
    "\n",
    "# X_test2 = X_test2_all[(np.argmax(Y_test2_all, axis=1) == 4)+(np.argmax(Y_test2_all, axis=1) == 0)]\n",
    "# Y_test2 = Y_test2_all[(np.argmax(Y_test2_all, axis=1) == 4)+(np.argmax(Y_test2_all, axis=1) == 0)][:,(0,4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## Use 5 classes as training data\n",
    "# X_train = X_train_all[(np.argmax(Y_train_all, axis=1) == 4)+(np.argmax(Y_train_all, axis=1) == 3)+\n",
    "#                       (np.argmax(Y_train_all, axis=1) == 2)+(np.argmax(Y_train_all, axis=1) == 8)+\n",
    "#                       (np.argmax(Y_train_all, axis=1) == 7)+(np.argmax(Y_train_all, axis=1) == 5)]\n",
    "# Y_train = Y_train_all[(np.argmax(Y_train_all, axis=1) == 4)+(np.argmax(Y_train_all, axis=1) == 3)+\n",
    "#                       (np.argmax(Y_train_all, axis=1) == 2)+(np.argmax(Y_train_all, axis=1) == 8)+\n",
    "#                       (np.argmax(Y_train_all, axis=1) == 7)+(np.argmax(Y_train_all, axis=1) == 5)][:,(2,3,4,5,8,7)]\n",
    "\n",
    "# X_valid = X_valid_all[(np.argmax(Y_valid_all, axis=1) == 4)+(np.argmax(Y_valid_all, axis=1) == 3)+\n",
    "#                       (np.argmax(Y_valid_all, axis=1) == 2)+(np.argmax(Y_valid_all, axis=1) == 8)+ \n",
    "#                       (np.argmax(Y_valid_all, axis=1) == 7)+(np.argmax(Y_valid_all, axis=1) == 5)]\n",
    "# Y_valid = Y_valid_all[(np.argmax(Y_valid_all, axis=1) == 4)+(np.argmax(Y_valid_all, axis=1) == 3)+\n",
    "#                       (np.argmax(Y_valid_all, axis=1) == 2)+(np.argmax(Y_valid_all, axis=1) == 8)+ \n",
    "#                       (np.argmax(Y_valid_all, axis=1) == 7)+(np.argmax(Y_valid_all, axis=1) == 5)][:,(2,3,4,5,8,7)]\n",
    "\n",
    "# X_test1 = X_test1_all[(np.argmax(Y_test1_all, axis=1) == 4)+(np.argmax(Y_test1_all, axis=1) == 3)+\n",
    "#                       (np.argmax(Y_test1_all, axis=1) == 2)+(np.argmax(Y_test1_all, axis=1) == 8)+\n",
    "#                       (np.argmax(Y_test1_all, axis=1) == 7)+(np.argmax(Y_test1_all, axis=1) == 5)]\n",
    "# Y_test1 = Y_test1_all[(np.argmax(Y_test1_all, axis=1) == 4)+(np.argmax(Y_test1_all, axis=1) == 3)+\n",
    "#                       (np.argmax(Y_test1_all, axis=1) == 2)+(np.argmax(Y_test1_all, axis=1) == 8)+\n",
    "#                       (np.argmax(Y_test1_all, axis=1) == 7)+(np.argmax(Y_test1_all, axis=1) == 5)][:,(2,3,4,5,8,7)]\n",
    "\n",
    "# X_test2 = X_test2_all[(np.argmax(Y_test2_all, axis=1) == 4)+(np.argmax(Y_test2_all, axis=1) == 3)+\n",
    "#                       (np.argmax(Y_test2_all, axis=1) == 2)+(np.argmax(Y_test2_all, axis=1) == 8)+\n",
    "#                       (np.argmax(Y_test2_all, axis=1) == 7)+(np.argmax(Y_test2_all, axis=1) == 5)]\n",
    "# Y_test2 = Y_test2_all[(np.argmax(Y_test2_all, axis=1) == 4)+(np.argmax(Y_test2_all, axis=1) == 3)+\n",
    "#                       (np.argmax(Y_test2_all, axis=1) == 2)+(np.argmax(Y_test2_all, axis=1) == 8)+\n",
    "#                       (np.argmax(Y_test2_all, axis=1) == 7)+(np.argmax(Y_test2_all, axis=1) == 5)][:,(2,3,4,5,8,7)]\n",
    "\n",
    "\n",
    "## Use the whole data set as training data\n",
    "\n",
    "X_train = X_train_all\n",
    "Y_train = Y_train_all\n",
    "\n",
    "X_valid = X_valid_all\n",
    "Y_valid = Y_valid_all\n",
    "\n",
    "X_test1 = X_test1_all\n",
    "Y_test1 = Y_test1_all\n",
    "\n",
    "X_test2 = X_test2_all\n",
    "Y_test2 = Y_test2_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_FLAT_SIZE = IMG_WIDTH*IMG_HEIGHT\n",
    "IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT)\n",
    "N_LABELS = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21728, 10)\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([2793,  765, 2763, 2027, 2321, 2704,  213, 2625, 2717, 2800]))\n"
     ]
    }
   ],
   "source": [
    "print (Y_train.shape)\n",
    "print (np.unique(np.argmax(Y_train, axis=1), return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21728, 16384)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Helper function\n",
    "\n",
    "## Weight initialization\n",
    "def new_weights(shape, stddev):\n",
    "    ## Xavier intialization\n",
    "    initial = tf.truncated_normal(shape=shape, stddev=stddev,dtype=tf.float32)\n",
    "    return tf.Variable(initial)\n",
    "## Biases initialization\n",
    "def new_biases(length):\n",
    "    initial = tf.constant(value=0, shape=[length], dtype=tf.float32)\n",
    "    return tf.Variable(initial)\n",
    "def get_layer_shape(in_layer):\n",
    "    shape = in_layer.get_shape()\n",
    "#     n_features = shape[1:4].num_elements()\n",
    "    return np.array([shape[1:2].num_elements(), shape[2:3].num_elements(), shape[3:4].num_elements()])\n",
    "\n",
    "## Fully connected layer\n",
    "def new_fc_layer(in_layer, n_in_features, n_out_features, relu=True):\n",
    "    stddev = np.sqrt(np.float(2)/(n_in_features + n_out_features))\n",
    "    \n",
    "    weights = new_weights([n_in_features, n_out_features], stddev)\n",
    "    biases = new_biases(n_out_features)\n",
    "    out_layer = tf.matmul(in_layer, weights) + biases\n",
    "    if relu:\n",
    "        out_layer = tf.nn.relu(out_layer)\n",
    "    return out_layer, weights\n",
    "#Helper function creates conv layer\n",
    "def new_conv_layer(in_layer, filter_height, filter_width, in_channels, n_filter, pooling=True):\n",
    "    in_layer_shape = get_layer_shape(in_layer)\n",
    "    in_features = np.prod(in_layer_shape[0:2])\n",
    "    if(pooling):\n",
    "        out_features = (in_layer_shape[0] - filter_height + 1)*(in_layer_shape[1] - filter_width + 1)/8\n",
    "                                    \n",
    "    else:\n",
    "        out_features = (in_layer_shape[0] - filter_height + 1)*(in_layer_shape[1] - filter_width + 1)\n",
    "        \n",
    "#     print in_features, out_features\n",
    "    stddev = np.sqrt(np.float(2)/(in_features+out_features))\n",
    "    weights = new_weights([filter_height, filter_width, in_channels, n_filter], stddev)\n",
    "    biases = new_biases(n_filter)\n",
    "    out_layer = tf.nn.conv2d(in_layer, weights, strides=[1,1,1,1], padding='VALID')\n",
    "    out_layer += biases\n",
    "    if pooling:\n",
    "        out_layer = tf.nn.max_pool(out_layer, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    out_layer = tf.nn.relu(out_layer)\n",
    "    return out_layer, weights\n",
    "def flatten_layer(in_layer):\n",
    "    shape = in_layer.get_shape()\n",
    "    n_features = shape[1:4].num_elements()\n",
    "    layer_flat = tf.reshape(in_layer, [-1, n_features])\n",
    "    return layer_flat, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 16384), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(?, 128, 128, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X_cnn = tf.placeholder(tf.float32, [None, IMG_FLAT_SIZE])\n",
    "X_cnn_train = tf.reshape(X_cnn, [-1, IMG_WIDTH, IMG_HEIGHT, 1])\n",
    "Y = tf.placeholder(tf.float32, [None, N_LABELS])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "print (X_cnn)\n",
    "print (X_cnn_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv Layer 0: Tensor(\"Reshape:0\", shape=(?, 128, 128, 1), dtype=float32)\n",
      "Conv Layer 1: Tensor(\"Relu:0\", shape=(?, 57, 63, 12), dtype=float32)\n",
      "Conv Layer 2: Tensor(\"Relu_1:0\", shape=(?, 21, 30, 24), dtype=float32)\n",
      "Conv Layer 3: Tensor(\"Relu_2:0\", shape=(?, 5, 27, 24), dtype=float32)\n",
      "Conv layer 3 after dropout: Tensor(\"dropout/mul:0\", shape=(?, 5, 27, 24), dtype=float32)\n",
      "Flatten layer: Tensor(\"Reshape_1:0\", shape=(?, 3240), dtype=float32), number of features: 3240\n"
     ]
    }
   ],
   "source": [
    "## 3 convolutional layers\n",
    "n_conv_layers = 3\n",
    "\n",
    "filter_heights = dict(zip(range(n_conv_layers+1), [0, 15, 16, 17]))\n",
    "filter_widths = dict(zip(range(n_conv_layers+1), [0, 2, 3, 4]))\n",
    "n_filters = dict(zip(range(n_conv_layers+1), [1, 12, 24, 24])) \n",
    "pooling = dict(zip(range(n_conv_layers+1), [False, True, True, False]))\n",
    "\n",
    "## Helper function to create convolultional layers\n",
    "def make_conv_layer(input_data, n_conv_layers, filter_heights, filter_widths, n_filters, dropout=True):\n",
    "    conv_layers = [0]*(n_conv_layers+1)\n",
    "    conv_layers[0] = input_data\n",
    "    weights = [0]*n_conv_layers\n",
    "    for f in range(1, n_conv_layers+1):\n",
    "        \n",
    "        conv_layers[f], weights[f-1] = new_conv_layer(in_layer=conv_layers[f-1], \n",
    "                                                          filter_height=filter_heights[f],\n",
    "                                                          filter_width=filter_widths[f],\n",
    "                                                          in_channels=n_filters[f-1], \n",
    "                                                          n_filter=n_filters[f], \n",
    "                                                          pooling=pooling[f])\n",
    "    if(dropout):\n",
    "        drop = tf.nn.dropout(conv_layers[n_conv_layers], keep_prob=keep_prob)\n",
    "        conv_layers.append(drop)\n",
    "    return (conv_layers, weights)\n",
    "\n",
    "conv_layers, conv_weights = make_conv_layer(input_data=X_cnn_train, \n",
    "                                            n_conv_layers=n_conv_layers, \n",
    "                                            filter_heights=filter_heights,\n",
    "                                            filter_widths=filter_widths,\n",
    "                                            n_filters=n_filters, \n",
    "                                            dropout=True)\n",
    "\n",
    "for l, layer in zip(range(n_conv_layers + 1), conv_layers):\n",
    "    print (\"Conv Layer {0}: {1}\".format(l, layer))\n",
    "print (\"Conv layer {1} after dropout: {0}\".format(conv_layers[-1], n_conv_layers))\n",
    "## Flatten layer\n",
    "layer_flat, n_features = flatten_layer(conv_layers[-1])\n",
    "print (\"Flatten layer: {0}, number of features: {1}\".format(layer_flat, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully connected layer 1: Tensor(\"Relu_3:0\", shape=(?, 256), dtype=float32)\n",
      "Fully connected layer 1 after droppout: Tensor(\"dropout_1/mul:0\", shape=(?, 256), dtype=float32)\n",
      "Fully connected layer 2: Tensor(\"add_4:0\", shape=(?, 10), dtype=float32)\n",
      "Softmax output: Tensor(\"Softmax:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## Fully-connected layer\n",
    "## Input: flatten layer resulted from conv layers\n",
    "fc_size = 256\n",
    "layer_fc1, weights_fc_1 = new_fc_layer(layer_flat, n_in_features=n_features, n_out_features=fc_size)\n",
    "print (\"Fully connected layer 1: {0}\".format(layer_fc1))\n",
    "\n",
    "layer_fc1_drop = tf.nn.dropout(layer_fc1, keep_prob=keep_prob)\n",
    "print (\"Fully connected layer 1 after droppout: {0}\".format(layer_fc1_drop))\n",
    "\n",
    "layer_fc2, weights_fc_2 = new_fc_layer(layer_fc1_drop, n_in_features=fc_size, n_out_features=N_LABELS, relu=False)\n",
    "print (\"Fully connected layer 2: {0}\".format(layer_fc2))\n",
    "\n",
    "# layer_fc2_drop = tf.nn.dropout(layer_fc2, keep_prob=keep_prob)\n",
    "# print (layer_fc2_drop)\n",
    "\n",
    "y_nn_softmax = tf.nn.softmax(layer_fc2)\n",
    "print (\"Softmax output: {0}\".format(y_nn_softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_4:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "(<tf.Tensor 'gradients_1/AddN_1:0' shape=(3240, 256) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fcbcfcf9710>)\n",
      "(<tf.Tensor 'gradients_1/AddN:0' shape=(256, 10) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fcbcfcdcad0>)\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "BETA = 1e-3\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2, labels=Y)\n",
    "print (cross_entropy)\n",
    "# cost = tf.reduce_mean(cross_entropy)\n",
    "cost = tf.reduce_mean(cross_entropy + BETA*tf.nn.l2_loss(weights_fc_1) +  BETA*tf.nn.l2_loss(weights_fc_2))\n",
    "print (cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "minimize_cost = optimizer.minimize(cost)\n",
    "\n",
    "grads = optimizer.compute_gradients(cost,[weights_fc_1, weights_fc_2])\n",
    "y_true = tf.argmax(Y, dimension=1)\n",
    "y_pred = tf.argmax(y_nn_softmax, dimension=1)\n",
    "\n",
    "print grads[0]\n",
    "print grads[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #slice data for testing model\n",
    "# X_train = X_train[:1000]\n",
    "# Y_train = Y_train[:1000]\n",
    "\n",
    "# print (X_train.shape)\n",
    "# print (Y_train.shape)\n",
    "# print (np.unique(np.argmax(Y_train, axis=1), return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_conv_weights(session, weights, weights_name, n_channels, epoch):\n",
    "    n_channels -= 1\n",
    "    \n",
    "    for name, weight in zip(weights_name, weights):\n",
    "        w = session.run(weight)\n",
    "        w_min = np.min(w)\n",
    "        w_max = np.max(w)\n",
    "        n_filters = w.shape[3]\n",
    "        height, width = w.shape[0:2]\n",
    "        n_grids = int(np.ceil(np.sqrt(n_filters)))\n",
    "        fig, axes = plt.subplots(n_grids, n_grids)\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            # Only plot the valid filter-weights.\n",
    "            if i<n_filters:\n",
    "                # Get the weights for the i'th filter of the input channel.\n",
    "                # See new_conv_layer() for details on the format\n",
    "                # of this 4-dim tensor.\n",
    "                img = w[:, :, n_channels, i]\n",
    "\n",
    "                # Plot image.\n",
    "                ax.imshow(img, \n",
    "                          vmin=w_min, vmax=w_max, \n",
    "                          cmap='nipy_spectral')\n",
    "                \n",
    "                \n",
    "            # Remove ticks from the plot.\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            fig.suptitle(name + ', epoch: ' + str(epoch)+', size: (' + str(height) + ',' + str(width) + ')', \n",
    "                         fontsize=10)\n",
    "            dir_path = './weights_img/'\n",
    "            if not os.path.exists(dir_path):\n",
    "                os.makedirs(dir_path)\n",
    "            fig.savefig(dir_path + name + ', epoch: ' + str(epoch) + '.png')\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classID = {\n",
    "    0: \"AirCon\",\n",
    "    1: \"Car horn\",\n",
    "    2: \"Children playing\",\n",
    "    3: \"Dog bark\",\n",
    "    4: \"Drilling\",\n",
    "    5: \"Engine idling\",\n",
    "    6: \"Gun shot\",\n",
    "    7: \"Jackhammer\",\n",
    "    8: \"Siren\",\n",
    "    9: \"Street music\"\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "TRAINING_EPOCHS = 3\n",
    "np.random.seed(2017)\n",
    "\n",
    "## Helper function for optimization\n",
    "def optimize(train_x, train_y, n_epochs, batch_size, session, saver):\n",
    "        n_samples = train_x.shape[0]\n",
    "        sample_IDs = np.arange(n_samples)\n",
    "        np.random.shuffle(sample_IDs)\n",
    "        train_x_p, train_y_p = train_x[sample_IDs], train_y[sample_IDs]\n",
    "        n_iterations = np.int(np.floor(n_samples/batch_size))+1\n",
    "        start_time = time.time()\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        print \"Training.......\"\n",
    "        print \"-- Elapsed time -- Epoch -- Cost value -- fc_1 median abs -- fc_2 median abs --\"\n",
    "\n",
    "        for epoch in np.arange(n_epochs+1):\n",
    "            np.random.shuffle(sample_IDs)\n",
    "            train_x_p, train_y_p = train_x[sample_IDs], train_y[sample_IDs]\n",
    "            for itr in np.arange(n_iterations):\n",
    "                start = (itr * batch_size) % (n_samples - batch_size)\n",
    "                batch_x, batch_y = train_x_p[start:start + batch_size], train_y_p[start:start + batch_size]\n",
    "                feed_dict_train = {X_cnn: batch_x, Y: batch_y, keep_prob: 0.5}\n",
    "                _, c = session.run([minimize_cost, cost], feed_dict=feed_dict_train)\n",
    "                g = session.run([grad for grad, _ in grads], feed_dict=feed_dict_train)\n",
    "                \n",
    "                \n",
    "            if(epoch % (n_epochs/10) == 0):\n",
    "                print \"-- {:12.6f} -- {:5d} -- {:10.5f} -- {:15.11f} -- {:15.11f} --\".format((time.time() - start_time), \n",
    "                                                                                    epoch, \n",
    "                                                                                    c, \n",
    "                                                                                    np.median(np.absolute(g[0])), \n",
    "                                                                                    np.median(np.absolute(g[1])))\n",
    "            cost_history = np.append(cost_history,c)\n",
    "#             Draw weights of convolutional layer\n",
    "            if(epoch % (n_epochs/3) == 0):\n",
    "                plot_conv_weights(session, [conv_weights[0], conv_weights[1], conv_weights[2]], \n",
    "                                  ['conv_1', 'conv_2', 'conv_3'], 1, epoch)\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#         Save model in folder cnn_model\n",
    "        dir_path = './cnn_model/'\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "        saver.save(sess, 'cnn_model/new_cnn')\n",
    "        \n",
    "#         print running time and output cost value graph\n",
    "        print (\"---Running time: %s seconds ---\" % (time.time() - start_time))\n",
    "        print ('*'*50)\n",
    "        fig = plt.figure(figsize=(10,5))\n",
    "        plt.plot(cost_history)\n",
    "        plt.axis([0,epoch,0,np.max(cost_history)])\n",
    "        plt.show()\n",
    "\n",
    "def output_log_file(train, valid, test1, test2):\n",
    "    file_name = pickle_file.split('/')[-1].split('.')[0]\n",
    "    with open(\"log/logfile.txt\", \"ab\") as text_file:\n",
    "        text_file.write('='*60)\n",
    "        text_file.write('\\n')\n",
    "        text_file.write(\"Time: {0}\\n\".format(datetime.now()))\n",
    "        text_file.write(\"Data: \" + file_name +\"\\n\")    \n",
    "        \n",
    "        \n",
    "#         text_file.write(\"Number of input samples: {:6d}\\n\".format(N_SA)\n",
    "        text_file.write(\"Number of input features: {:5d}\\n\".format(IMG_FLAT_SIZE))\n",
    "#         text_file.write(\"Number of input labels: {:5d}\\n\".format(N_LABELS)\n",
    "        text_file.write(\"Number of convolutional layer: {0}\\n\".format(n_conv_layers))\n",
    "        text_file.write(\"\\tFilter size:\\t\")\n",
    "        for idx in np.arange(1, n_conv_layers + 1):\n",
    "            text_file.write('{0}\\t'.format(filter_size[idx]))\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"\\tNumber of filter:\\t\")\n",
    "        for idx in np.arange(1, n_conv_layers + 1):\n",
    "            text_file.write('{0}\\t'.format(n_filter[idx]))\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"Number of fully-connected layer: {0}\\n\".format(n_fc_layers))\n",
    "        text_file.write(\"Hidden units: {:3d}  - {:3d}\\n\".format(fc_size, N_LABELS))\n",
    "        text_file.write(\"Training epochs: {:5d}\\n\".format(TRAINING_EPOCHS))\n",
    "        text_file.write(\"Batch size: {:3d}\\n\".format(BATCH_SIZE))\n",
    "                        \n",
    "        text_file.write('\\tTrain\\tValid\\tTest1\\tYoutube\\n')\n",
    "        text_file.write(\"Fscore\\t {:1.2f} \\t {:1.2f} \\t {:1.2f} \\t {:1.2f} \\n\".format(train[0], \n",
    "                                                                                      valid[0], \n",
    "                                                                                      test1[0], \n",
    "                                                                                      test2[0]))\n",
    "        text_file.write(\"Acc\\t {:1.2f} \\t {:1.2f} \\t {:1.2f} \\t {:1.2f} \\n\".format(train[1], \n",
    "                                                                                   valid[1], \n",
    "                                                                                   test1[1], \n",
    "                                                                                   test2[1]))\n",
    "        \n",
    "        \n",
    "\n",
    "## Helper function to print confusion matrix\n",
    "def make_prediction(test_x, test_y, session, batch_size):\n",
    "    print \"Making prediction.......\"\n",
    "    start_time = time.time()\n",
    "    n_samples = test_x.shape[0]\n",
    "    sample_IDs = np.arange(n_samples)\n",
    "    n_iterations = np.int(np.floor(n_samples/batch_size))+1\n",
    "    pred = np.zeros(n_samples)\n",
    "    true = np.zeros(n_samples)\n",
    "    for itr in np.arange(n_iterations):\n",
    "        start = (itr * batch_size) % (n_samples - batch_size)\n",
    "        batch_x, batch_y = test_x[start:start + batch_size], test_y[start:start + batch_size]\n",
    "        feed_dict_test = {X_cnn: batch_x, Y: batch_y, keep_prob: 1}\n",
    "        pred[start:start + batch_size], true[start:start + batch_size] = session.run([y_pred, y_true], feed_dict=feed_dict_test)\n",
    "\n",
    "## Performance Evaluation metrics\n",
    "    ## Accuracy\n",
    "    accuracy = accuracy_score(true, pred)\n",
    "    print (\"Accuracy: {:3.2f}\".format(accuracy))\n",
    "    ## F-score\n",
    "    if(N_LABELS == 2):\n",
    "        p,r,f,s = precision_recall_fscore_support(true, pred, average='binary')\n",
    "    else:\n",
    "        p,r,f,s = precision_recall_fscore_support(true, pred, average='macro')\n",
    "    print (\"F-Score: {:3.2f}\".format(f))\n",
    "    \n",
    "    ## Confusion matrix\n",
    "    print (\"Confusion Matrix\")\n",
    "    print confusion_matrix(true, pred)\n",
    "#     print (\"Analyzing result\")\n",
    "#     for ID, name in classID.items():\n",
    "#         x = pred[(true == ID)*(pred != ID)]  \n",
    "#         unique_class, unique_count = np.unique(x, return_counts=True)\n",
    "#         print ('-'*50)\n",
    "#         print \" Class {0} is mistaken with: \".format(name)\n",
    "#         for c, y in zip(unique_class, unique_count):        \n",
    "#             print \"# {0}: {1} samples\".format(classID[c], y)\n",
    "#     print ('-'*50)\n",
    "    print (\"---Running time: {0} seconds ---\".format((time.time() - start_time)))\n",
    "    print ('*'*50)\n",
    "    return accuracy, f\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.......\n",
      "-- Elapsed time -- Epoch -- Cost value -- fc_1 median abs -- fc_2 median abs --\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    optimize(X_train, Y_train, TRAINING_EPOCHS, BATCH_SIZE, sess, saver)\n",
    "    \n",
    "    print (\"Making prediction on training set\")\n",
    "    train = make_prediction(X_train, Y_train, sess, BATCH_SIZE)\n",
    "    print (\"Making prediction on validation set\")\n",
    "    valid = make_prediction(X_valid, Y_valid, sess, BATCH_SIZE)\n",
    "    print (\"Making training prediction on test 1 set\")\n",
    "    test1 = make_prediction(X_test1, Y_test1, sess, BATCH_SIZE)\n",
    "    print (\"Making training prediction on test 2 set\")\n",
    "    test2 = make_prediction(X_test2, Y_test2, sess, BATCH_SIZE)\n",
    "#     output_log_file(train, valid, test1, test2)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
