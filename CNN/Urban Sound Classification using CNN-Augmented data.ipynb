{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import specgram\n",
    "import time\n",
    "from sklearn.cross_validation import train_test_split\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 9\n",
    "plt.rcParams['ytick.labelsize'] = 9\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "plt.rcParams['figure.titlesize'] = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TENSORFLOW GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Log scaled melspectrogram parameters\n",
    "BANDS = 128\n",
    "FRAMES = 128\n",
    "# Number of classification classes\n",
    "N_LABELS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    \"\"\"Initialize 4-D weight using normal distribution with mean = 0, stddev = 0.1\n",
    "    Args:\n",
    "        shape (4-D list_like): [height, width, in_channels, out_channels] shape of weight\n",
    "    Return\n",
    "        weight (a tensor variables)\n",
    "    \"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def new_biases(length):\n",
    "    \"\"\"Initialize 1-D bias\n",
    "    Args:\n",
    "        lenght (int): length of bias\n",
    "    Return\n",
    "        bias (a tensor)\n",
    "    \"\"\"\n",
    "    initial = tf.constant(1.0, shape = [length])\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   use_pooling=True):  # Use 4x2 max-pooling.\n",
    "    \"\"\"Construct a convolutional layer by convoluting filter with input followed by a ReLu activation function\n",
    "    by default and max pooling by choice\n",
    "    Args:\n",
    "        input (4-D tensor): the previous layer\n",
    "        num_input_channels (int): number of channels in prev. layer\n",
    "        filter_size (int): Width and height of each filter.\n",
    "        num_filters (int): number of filters\n",
    "        use_pooling (bool): to choose whether to use max pooling or not\n",
    "    Return:\n",
    "        layer (4-D tensor): convolutional layer\n",
    "        weight (4-D tensor)\n",
    "    \"\"\"\n",
    "\n",
    "    # Shape of the filter-weights for the convolution.\n",
    "    # This format is determined by the TensorFlow API.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights with the given shape.\n",
    "    weights = new_weights(shape=shape)\n",
    "\n",
    "    # Create new biases, one for each weight.\n",
    "    biases = new_biases(length=num_filters)\n",
    "\n",
    "    # Create the TensorFlow operation for convolution.\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='VALID')\n",
    "\n",
    "    # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    layer += biases\n",
    "\n",
    "    # Use pooling to down-sample the image resolution?\n",
    "    if use_pooling:\n",
    "        # This is 4x2 max-pooling, which means that we\n",
    "        # consider 4x2 windows and select the largest value\n",
    "        # in each window. \n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 4, 2, 1],\n",
    "                               strides=[1, 4, 2, 1],\n",
    "                               padding='VALID')\n",
    "\n",
    "    # Rectified Linear Unit (ReLU).\n",
    "    # It calculates max(x, 0) for each input pixel x.\n",
    "    # This adds some non-linearity to the formula and allows us\n",
    "    # to learn more complicated functions.\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Note that ReLU is normally executed before the pooling,\n",
    "    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "    # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "    # We return both the resulting layer and the filter-weights\n",
    "    # because we will plot the weights later.\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    \"\"\"Flatten a layer into a 2-D shape\n",
    "    Args:\n",
    "        layer (more than 2-D tensor): the previous layer\n",
    "    Return:\n",
    "        layer_flat (2-D tensor): flaten version of layer\n",
    "        n_features (int): number of features after flattenning\n",
    "    \"\"\"\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is expected to be:\n",
    "    # layer_shape == [n_samples, height, width, n_channels]\n",
    "\n",
    "    # The number of features is: height * width * n_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    n_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [n_samples, n_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, n_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [n_samples, height * width * n_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def new_fc_layer(input_layer,    # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 activation=\"\"): # Use Rectified Linear Unit (ReLU)?\n",
    "    \"\"\"Compute a fully connected layer\n",
    "    Args:\n",
    "        input_layer (2-D tensor): the previous layer\n",
    "        num_inputs (int): Num. inputs features from prev. layer.\n",
    "        num_outputs (int): Num. output features\n",
    "        activation (string): type of activation function\n",
    "    Return:\n",
    "        layer (2-D tensor): output layer\n",
    "        weight (2-D tensor)\n",
    "    \"\"\"           \n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input_layer, weights) + biases\n",
    "    activation = activation.lower()\n",
    "    if (activation ==\"sigmoid\"):\n",
    "        layer = tf.nn.sigmoid(layer)\n",
    "    elif (activation ==\"relu\"):\n",
    "        layer = tf.nn.relu(layer)    \n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, BANDS, FRAMES, 1])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, N_LABELS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convolution layer 1 params\n",
    "filter_l1_size = 5\n",
    "n_filter_l1 = 24\n",
    "stride_l1 = [1, 4, 2, 1]\n",
    "activation_func_l1 = \"relu\"\n",
    "layer_conv1, weights_1 = new_conv_layer(input=X, \n",
    "                                        num_input_channels=1, \n",
    "                                        filter_size=filter_l1_size, \n",
    "                                        num_filters=n_filter_l1, \n",
    "                                        use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu:0' shape=(?, 31, 62, 24) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convolution layer 2 params\n",
    "filter_l2_size = 5\n",
    "n_filter_l2 = 48\n",
    "stride_l2 = [1, 4, 2, 1]\n",
    "activation_func_l2 = \"relu\"\n",
    "layer_conv2, weights_2 = new_conv_layer(input=layer_conv1, \n",
    "                                        num_input_channels=n_filter_l1, \n",
    "                                        filter_size=filter_l2_size, \n",
    "                                        num_filters=n_filter_l2, \n",
    "                                        use_pooling=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_1:0' shape=(?, 6, 29, 48) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convolution layer 3 params\n",
    "filter_l3_size = 5\n",
    "n_filter_l3 = 48\n",
    "activation_func_l3 = \"relu\"\n",
    "\n",
    "layer_conv3, weights_3 = new_conv_layer(input=layer_conv2, \n",
    "                                        num_input_channels=n_filter_l2, \n",
    "                                        filter_size=filter_l3_size, \n",
    "                                        num_filters=n_filter_l3, \n",
    "                                        use_pooling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_2:0' shape=(?, 2, 25, 48) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input dropout 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "layer_conv3_drop = tf.nn.dropout(layer_conv3, keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout/mul:0' shape=(?, 2, 25, 48) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv3_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_flat, num_features = flatten_layer(layer_conv3_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(?, 2400) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fully connected layer 4\n",
    "num_hidden = 64\n",
    "activation_func_l4 = \"relu\"\n",
    "layer_fc1, weights_fc1 = new_fc_layer(layer_flat, num_inputs=num_features, num_outputs=num_hidden, activation=activation_func_l4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_3:0' shape=(?, 64) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Dropout 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_fc1_drop = tf.nn.dropout(layer_fc1, keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_1/mul:0' shape=(?, 64) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc1_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_fc2, weights_fc2 = new_fc_layer(layer_fc1_drop, num_inputs=num_hidden, num_outputs=N_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_4:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(layer_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BETA = 0.001 #L2 regularization penalty factor\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,labels=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##L2 regularization\n",
    "cost = tf.reduce_mean(cross_entropy + BETA*tf.nn.l2_loss(weights_fc1) +  BETA*tf.nn.l2_loss(weights_fc2))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_PARTS = 10\n",
    "def load_train_data(parent_dirs):\n",
    "    print \"Load train data\"\n",
    "    print \"Loading.........\"\n",
    "    start_time = time.time()\n",
    "    train_features = np.zeros(0)\n",
    "    train_labels = np.zeros(0)\n",
    "    train_names = np.zeros(0)\n",
    "    \n",
    "    \n",
    "    for part in np.arange(N_PARTS-1):\n",
    "        for parent_dir in parent_dirs:\n",
    "            train_features = np.concatenate([train_features, np.load(parent_dir + \"/cnn_train_features_full_part_\" + str(part+1) + \".npy\", allow_pickle=True)], axis=0)  \n",
    "            train_labels = np.concatenate([train_labels, np.load(parent_dir + \"/cnn_train_labels_full_part_\" + str(part+1) + \".npy\", allow_pickle=True)], axis=0)      \n",
    "            train_names = np.concatenate([train_names, np.load(parent_dir + \"/cnn_train_file_names_full_part_\" + str(part+1) + \".npy\", allow_pickle=True)], axis=0)\n",
    "        \n",
    "        print \"Load part {0} successfully\".format(part+1)\n",
    "    print \"---Running time: {0} seconds ---\".format(time.time() - start_time)\n",
    "    return train_features, train_labels, train_names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train data\n",
      "Loading.........\n",
      "Load part 1 successfully\n",
      "Load part 2 successfully\n",
      "Load part 3 successfully\n",
      "Load part 4 successfully\n",
      "Load part 5 successfully\n",
      "Load part 6 successfully\n",
      "Load part 7 successfully\n",
      "Load part 8 successfully\n",
      "Load part 9 successfully\n",
      "---Running time: 101.455986023 seconds ---\n"
     ]
    }
   ],
   "source": [
    "test_features = None\n",
    "test_labels = None\n",
    "test_names = None\n",
    "test_IDs = None\n",
    "parent_dirs = [\"train_data\"]\n",
    "train_features, train_labels, train_names = load_train_data(parent_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22144,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_frames(data):\n",
    "    total_frames = 0\n",
    "    frame_ID = []\n",
    "    d_length = np.shape(data)[0]\n",
    "    for i,d in zip(np.arange(d_length),data):\n",
    "        length = np.shape(d)[0]\n",
    "        n_frames = (length/BANDS-FRAMES)\n",
    "        total_frames = total_frames + n_frames\n",
    "        for _ in np.arange(n_frames):\n",
    "            frame_ID.append(i)\n",
    "    return total_frames, np.asarray(frame_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_one_hot = one_hot_encode(train_labels.astype(np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_one_hot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_train_x, _, dummy_train_y, _ = train_test_split(train_features, train_one_hot, test_size=0.99, random_state=4, stratify=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt, idi = count_frames(dummy_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch_patch(data, data_labels):\n",
    "    patches = []\n",
    "    labels = []\n",
    "    for l, d in zip(data_labels.astype(np.int), data):\n",
    "        length = np.shape(d)[0]\n",
    "        p = np.reshape(d, (BANDS, length/BANDS))\n",
    "        n_frames = np.shape(p)[1] - FRAMES\n",
    "        for f in np.arange(n_frames):\n",
    "            patches.append(p[:,f:f+FRAMES])\n",
    "            labels.append(l)\n",
    "    features = np.asanyarray(patches, dtype=np.float32).reshape(np.shape(patches)[0],BANDS,FRAMES,1)\n",
    "    labels = np.asanyarray(labels, dtype=np.float32)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummy_train_x, dummy_train_y = fetch_patch(dummy_train_x, dummy_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31487, 128, 128, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(dummy_train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "BATCH_SIZE = 100\n",
    "training_epochs = 300\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "session = tf.Session()\n",
    "    \n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to perform optimization iterations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummy_optimize(num_epochs, train_x, train_y):\n",
    "    start_time = time.time()\n",
    "    cost_history = np.empty(shape=[1],dtype=float)\n",
    "    print \"Training.......\"\n",
    "    print \"------ Elapsed time ------- Epoch ---- Cost \"\n",
    "    for itr in range(num_epochs):\n",
    "        offset = (itr * BATCH_SIZE) % (train_y.shape[0] - BATCH_SIZE)\n",
    "        batch_x = train_x[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "        batch_y = train_y[offset:(offset + BATCH_SIZE), :]\n",
    "        feed_dict_train={X: batch_x, Y : batch_y, keep_prob : 0.5}\n",
    "        _, c = session.run([optimizer, cost],feed_dict=feed_dict_train)\n",
    "        if(itr % 100 == 0):\n",
    "            print \"------ {:12.7f} ------- {:5d} ---- {:12.10f} \".format((time.time() - start_time), itr, c)\n",
    "        cost_history = np.append(cost_history,c)\n",
    "    print (\"---Training time: %s seconds ---\" % (time.time() - start_time))\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    plt.plot(cost_history)\n",
    "    plt.axis([0,itr,0,np.max(cost_history)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize(num_epochs, train_x, train_y):\n",
    "    print \"Training.......\"\n",
    "    print \"------ Elapsed time ------- Epoch ---- Cost \"\n",
    "    cost_history = np.empty(shape=[1],dtype=float)\n",
    "    n_samples = np.shape(train_x)[0]\n",
    "    iterations = np.int32(np.floor(n_samples/(8*BATCH_SIZE)) + 1);\n",
    "    samples = np.arange(n_samples)\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):    \n",
    "   \n",
    "        \n",
    "        np.random.shuffle(samples)\n",
    "        for itr in np.arange(iterations):\n",
    "            offset = itr*BATCH_SIZE \n",
    "            batch = samples[offset:offset+BATCH_SIZE]\n",
    "\n",
    "            batch_x = train_x[batch]\n",
    "            batch_y = train_y[batch]\n",
    "            feed_dict_train={X: batch_x, Y : batch_y, keep_prob : 0.5}\n",
    "            _, c = session.run([optimizer, cost],feed_dict=feed_dict_train)\n",
    "        if(epoch % 5 == 0):\n",
    "            print \"------ {:12.7f} ------- {:5d} ---- {:12.10f} \".format((time.time() - start_time), epoch, c)\n",
    "        cost_history = np.append(cost_history,c)\n",
    "    print (\"---Training time: %s seconds ---\" % (time.time() - start_time))\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    plt.plot(cost_history)\n",
    "    plt.axis([0,epoch,0,np.max(cost_history)])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummy_optimize(training_epochs, dummy_train_x, dummy_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize(training_epochs, dummy_train_x, dummy_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummy_load_test_data(parent_dirs):\n",
    "    print \"Load test data\"\n",
    "    print \"Loading.........\"\n",
    "    start_time = time.time()\n",
    "    test_features = np.zeros((0, 128, 128, 1))\n",
    "    test_labels = np.zeros(0)\n",
    "    test_names = np.zeros(0)\n",
    "    test_id = np.zeros(0)\n",
    "    \n",
    "    \n",
    "    for fold in np.arange(2):\n",
    "        for parent_dir in parent_dirs:\n",
    "            test_features = np.concatenate([test_features, np.load(parent_dir + \"/cnn_test_features_full_fold_\" + str(fold+1) + \".npy\", allow_pickle=True)], axis=0)  \n",
    "            test_labels = np.concatenate([test_labels, np.load(parent_dir + \"/cnn_test_labels_full_fold_\" + str(fold+1) + \".npy\", allow_pickle=True)], axis=0)      \n",
    "            test_names = np.concatenate([test_names, np.load(parent_dir + \"/cnn_test_file_names_full_fold_\" + str(fold+1) + \".npy\", allow_pickle=True)], axis=0)\n",
    "            test_id = np.concatenate([test_id, np.load(parent_dir + \"/cnn_test_ID_full_fold_\" + str(fold+1) + \".npy\", allow_pickle=True)], axis=0)\n",
    "        \n",
    "        print \"Load fold {0} successfully\".format(fold+1)\n",
    "    print \"---Running time: {0} seconds ---\".format(time.time() - start_time)\n",
    "    return test_features, test_labels, test_names, test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = None\n",
    "train_labels = None\n",
    "train_names = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_train_x = None\n",
    "dummy_train_y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dummy_test_x, dummy_test_y, dummy_test_names, dummy_test_IDs = dummy_load_test_data([\"test_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(dummy_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test_data(parent_dirs):\n",
    "    print \"Load test data\"\n",
    "    print \"Loading.........\"\n",
    "    start_time = time.time()\n",
    "    test_features = np.zeros((0, 128, 128, 1))\n",
    "    test_labels = np.zeros(0)\n",
    "    test_names = np.zeros(0)\n",
    "    test_id = np.zeros(0)\n",
    "    \n",
    "    \n",
    "    for fold in np.arange(20):\n",
    "        for parent_dir in parent_dirs:\n",
    "            test_features = np.concatenate([test_features, np.load(parent_dir + \"/cnn_test_features_full_fold_\" + str(fold+1) + \".npy\", allow_pickle=True)], axis=0)  \n",
    "            test_labels = np.concatenate([test_labels, np.load(parent_dir + \"/cnn_test_labels_full_fold_\" + str(fold+1) + \".npy\", allow_pickle=True)], axis=0)      \n",
    "            test_names = np.concatenate([test_names, np.load(parent_dir + \"/cnn_test_file_names_full_fold_\" + str(fold+1) + \".npy\", allow_pickle=True)], axis=0)\n",
    "            test_id = np.concatenate([test_id, np.load(parent_dir + \"/cnn_test_ID_full_fold_\" + str(fold+1) + \".npy\", allow_pickle=True)], axis=0)\n",
    "        \n",
    "        print \"Load fold {0} successfully\".format(fold+1)\n",
    "    print \"---Running time: {0} seconds ---\".format(time.time() - start_time)\n",
    "    return test_features, test_labels, test_names, test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_features = None\n",
    "train_labels = None\n",
    "train_names = None\n",
    "\n",
    "test_features, test_labels, test_names, test_IDs = load_test_data([\"test_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to make prediction for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_prediction(test_f, test_l, test_ID):\n",
    "    ID_list = np.unique(test_ID)\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for sound_ID in ID_list:\n",
    "        indices = np.where(test_ID == sound_ID)\n",
    "        input_x = np.float32(test_f[indices])\n",
    "        true_y = test_l[indices].astype(int)[0]\n",
    "        feed_dict_test = {X: input_x, keep_prob: 1.0}\n",
    "        pred_y = session.run(y_,feed_dict=feed_dict_test)\n",
    "        pred_y = np.mean(pred_y, axis=0)\n",
    "        pred_y = np.argmax(pred_y)\n",
    "        true_y = np.argmax(true_y)\n",
    "        y_pred.append(pred_y)\n",
    "        y_true.append(true_y)\n",
    "    return np.array(y_pred), np.array(y_true)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dummy test\n",
    "y_pred, y_true = make_prediction(dummy_test_x, one_hot_encode(dummy_test_y.astype(np.int)), dummy_test_IDs)\n",
    "p,r,f,s = precision_recall_fscore_support(y_true, y_pred, average='micro') \n",
    "print \"F-Score:\", round(f,3)\n",
    "print \"Confusion Matrix: \"\n",
    "print confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = np.where(dummy_test_IDs == 5)\n",
    "ipu = np.float32(dummy_test_x[i])\n",
    "pred_y = session.run(y_, feed_dict={X: ipu, keep_prob:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_y = np.sum(pred_y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argmax(dummy_test_y[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p,r,f,s = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    \n",
    "print \"F-Score:\", round(f,3)\n",
    "print \"Confusion Matrix: \"\n",
    "print confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p,r,f,s = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    \n",
    "print \"F-Score:\", round(f,3)\n",
    "print \"Confusion Matrix: \"\n",
    "print confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = np.where(test_IDs == 941)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ipu = test_features[i].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true = one_hot_encode(test_labels.astype(np.int))[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argmax(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(test_labels.astype(np.int))[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = session.run(y_,feed_dict={X: ipu, keep_prob:0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argmax(np.mean(pred, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
