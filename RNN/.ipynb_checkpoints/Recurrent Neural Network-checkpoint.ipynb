{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cPickle as pickle\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "plt.rcParams['figure.titlesize'] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = \"../../my_features/US8K_logmel128_patchslice.pickle\"\n",
    "with open(pickle_file, \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "    X_train_all = dataset[\"X_train\"]/10\n",
    "    Y_train_all = dataset[\"Y_train\"]\n",
    "    X_valid_all = dataset[\"X_valid\"]/10\n",
    "    Y_valid_all = dataset[\"Y_valid\"]\n",
    "    X_test1_all = dataset[\"X_test1\"]/10\n",
    "    Y_test1_all = dataset[\"Y_test1\"]\n",
    "    X_test2_all = dataset[\"X_test2\"]/10\n",
    "    Y_test2_all = dataset[\"Y_test2\"]\n",
    "    del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2793.   765.  2763.  2027.  2321.  2704.   213.  2625.  2717.  2800.]\n",
      "21728.0\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([2793,  765, 2763, 2027, 2321, 2704,  213, 2625, 2717, 2800]))\n",
      "('training data: ', (21728, 16384), (21728, 10))\n",
      "[ 2793.   765.  2763.  2027.  2321.  2704.   213.  2625.  2717.  2800.]\n"
     ]
    }
   ],
   "source": [
    "print np.sum(Y_train_all, axis = 0)\n",
    "print np.sum(Y_train_all)\n",
    "print np.unique(np.argmax(Y_train_all, axis=1), return_counts=True)\n",
    "print ('training data: ' , X_train_all.shape, Y_train_all.shape)\n",
    "print (np.sum(Y_train_all, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Use 2 classes as training data\n",
    "X_train = X_train_all[(np.argmax(Y_train_all, axis=1) == 4)+(np.argmax(Y_train_all, axis=1) == 0)]\n",
    "Y_train = Y_train_all[(np.argmax(Y_train_all, axis=1) == 4)+(np.argmax(Y_train_all, axis=1) == 0)][:,(0,4)]\n",
    "\n",
    "X_valid = X_valid_all[(np.argmax(Y_valid_all, axis=1) == 4)+(np.argmax(Y_valid_all, axis=1) == 0)]\n",
    "Y_valid = Y_valid_all[(np.argmax(Y_valid_all, axis=1) == 4)+(np.argmax(Y_valid_all, axis=1) == 0)][:,(0,4)]\n",
    "\n",
    "X_test1 = X_test1_all[(np.argmax(Y_test1_all, axis=1) == 4)+(np.argmax(Y_test1_all, axis=1) == 0)]\n",
    "Y_test1 = Y_test1_all[(np.argmax(Y_test1_all, axis=1) == 4)+(np.argmax(Y_test1_all, axis=1) == 0)][:,(0,4)]\n",
    "\n",
    "X_test2 = X_test2_all[(np.argmax(Y_test2_all, axis=1) == 4)+(np.argmax(Y_test2_all, axis=1) == 0)]\n",
    "Y_test2 = Y_test2_all[(np.argmax(Y_test2_all, axis=1) == 4)+(np.argmax(Y_test2_all, axis=1) == 0)][:,(0,4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Use 5 classes as training data\n",
    "# X_train = X_train_all[(np.argmax(Y_train_all, axis=1) == 4)+(np.argmax(Y_train_all, axis=1) == 3)+\n",
    "#                       (np.argmax(Y_train_all, axis=1) == 2)+(np.argmax(Y_train_all, axis=1) == 8)+\n",
    "#                       (np.argmax(Y_train_all, axis=1) == 7)+(np.argmax(Y_train_all, axis=1) == 5)]\n",
    "# Y_train = Y_train_all[(np.argmax(Y_train_all, axis=1) == 4)+(np.argmax(Y_train_all, axis=1) == 3)+\n",
    "#                       (np.argmax(Y_train_all, axis=1) == 2)+(np.argmax(Y_train_all, axis=1) == 8)+\n",
    "#                       (np.argmax(Y_train_all, axis=1) == 7)+(np.argmax(Y_train_all, axis=1) == 5)][:,(2,3,4,5,8,7)]\n",
    "\n",
    "# X_valid = X_valid_all[(np.argmax(Y_valid_all, axis=1) == 4)+(np.argmax(Y_valid_all, axis=1) == 3)+\n",
    "#                       (np.argmax(Y_valid_all, axis=1) == 2)+(np.argmax(Y_valid_all, axis=1) == 8)+ \n",
    "#                       (np.argmax(Y_valid_all, axis=1) == 7)+(np.argmax(Y_valid_all, axis=1) == 5)]\n",
    "# Y_valid = Y_valid_all[(np.argmax(Y_valid_all, axis=1) == 4)+(np.argmax(Y_valid_all, axis=1) == 3)+\n",
    "#                       (np.argmax(Y_valid_all, axis=1) == 2)+(np.argmax(Y_valid_all, axis=1) == 8)+ \n",
    "#                       (np.argmax(Y_valid_all, axis=1) == 7)+(np.argmax(Y_valid_all, axis=1) == 5)][:,(2,3,4,5,8,7)]\n",
    "\n",
    "# X_test1 = X_test1_all[(np.argmax(Y_test1_all, axis=1) == 4)+(np.argmax(Y_test1_all, axis=1) == 3)+\n",
    "#                       (np.argmax(Y_test1_all, axis=1) == 2)+(np.argmax(Y_test1_all, axis=1) == 8)+\n",
    "#                       (np.argmax(Y_test1_all, axis=1) == 7)+(np.argmax(Y_test1_all, axis=1) == 5)]\n",
    "# Y_test1 = Y_test1_all[(np.argmax(Y_test1_all, axis=1) == 4)+(np.argmax(Y_test1_all, axis=1) == 3)+\n",
    "#                       (np.argmax(Y_test1_all, axis=1) == 2)+(np.argmax(Y_test1_all, axis=1) == 8)+\n",
    "#                       (np.argmax(Y_test1_all, axis=1) == 7)+(np.argmax(Y_test1_all, axis=1) == 5)][:,(2,3,4,5,8,7)]\n",
    "\n",
    "# X_test2 = X_test2_all[(np.argmax(Y_test2_all, axis=1) == 4)+(np.argmax(Y_test2_all, axis=1) == 3)+\n",
    "#                       (np.argmax(Y_test2_all, axis=1) == 2)+(np.argmax(Y_test2_all, axis=1) == 8)+\n",
    "#                       (np.argmax(Y_test2_all, axis=1) == 7)+(np.argmax(Y_test2_all, axis=1) == 5)]\n",
    "# Y_test2 = Y_test2_all[(np.argmax(Y_test2_all, axis=1) == 4)+(np.argmax(Y_test2_all, axis=1) == 3)+\n",
    "#                       (np.argmax(Y_test2_all, axis=1) == 2)+(np.argmax(Y_test2_all, axis=1) == 8)+\n",
    "#                       (np.argmax(Y_test2_all, axis=1) == 7)+(np.argmax(Y_test2_all, axis=1) == 5)][:,(2,3,4,5,8,7)]\n",
    "\n",
    "\n",
    "## Use the whole data set as training data\n",
    "\n",
    "# X_train = X_train_all\n",
    "# Y_train = Y_train_all\n",
    "\n",
    "# X_valid = X_valid_all\n",
    "# Y_valid = Y_valid_all\n",
    "\n",
    "# X_test1 = X_test1_all\n",
    "# Y_test1 = Y_test1_all\n",
    "\n",
    "# X_test2 = X_test2_all\n",
    "# Y_test2 = Y_test2_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_FLAT_SIZE = IMG_WIDTH*IMG_HEIGHT\n",
    "IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT)\n",
    "N_LABELS = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5114, 2)\n",
      "(array([0, 1]), array([2793, 2321]))\n",
      "(5114, 16384)\n"
     ]
    }
   ],
   "source": [
    "print (Y_train.shape)\n",
    "print (np.unique(np.argmax(Y_train, axis=1), return_counts=True))\n",
    "print (X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recurrent Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## params\n",
    "chunk_size = 128 # n_bands\n",
    "n_chunks = 128 # n_frames\n",
    "hidden_size = 300 # lstm weight size\n",
    "batch_size = 50 # SGD batch size\n",
    "n_layers = 2 # Number of LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## helper function creating layers\n",
    "\n",
    "def new_weights(shape, stddev):\n",
    "    ## Xavier intialization\n",
    "    initial = tf.truncated_normal(shape=shape, stddev=stddev,dtype=tf.float32)\n",
    "    \n",
    "    return tf.Variable(initial)\n",
    "## Biases initialization\n",
    "def new_biases(length):\n",
    "    initial = tf.constant(value=0, shape=[length], dtype=tf.float32)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def new_layer(in_size, out_size):\n",
    "    stddev = np.sqrt(np.float(2)/(in_size + out_size))\n",
    "    weights = new_weights([in_size, out_size], stddev)\n",
    "    biases = new_biases(out_size)\n",
    "    return {'weights': weights, 'biases':biases}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 16384), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(?, 128, 128), dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## Placeholder variables to hold input data\n",
    "X = tf.placeholder(tf.float32, [None, IMG_FLAT_SIZE]) # Input data\n",
    "X_rnn = tf.reshape(X, [-1, n_chunks, chunk_size]) # Reshape input data\n",
    "Y = tf.placeholder(tf.float32, [None, N_LABELS]) # Labels\n",
    "print X\n",
    "print X_rnn\n",
    "print Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Layer: {'weights': <tensorflow.python.ops.variables.Variable object at 0x7f411875c790>, 'biases': <tensorflow.python.ops.variables.Variable object at 0x7f411c0bead0>}\n",
      "* rnn_cells: <tensorflow.python.ops.rnn_cell.MultiRNNCell object at 0x7f411c0beb50>\n",
      "* output transpose: Tensor(\"RNN/transpose:0\", shape=(?, 128, 300), dtype=float32)\n",
      "* outputs: Tensor(\"transpose_1:0\", shape=(128, ?, 300), dtype=float32)\n",
      "* mean_step: Tensor(\"Mean:0\", shape=(?, 300), dtype=float32)\n",
      "* input_softmax: Tensor(\"add:0\", shape=(?, 2), dtype=float32)\n",
      "* y_rnn_softmax: Tensor(\"Softmax:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "layer = new_layer(hidden_size, N_LABELS) # Softmax layer, outmost layer of the network\n",
    "print ('* Layer: {0}'.format(layer))\n",
    "\n",
    "# hidden_1_layer = new_layer(hidden_size, 100) # Hidden layer 1, outmost layer of the network\n",
    "# print ('* Hidden Layer 1: {0}'.format(hidden_1_layer))\n",
    "\n",
    "# softmax_layer = new_layer(100, N_LABELS) # Softmax layer, outmost layer of the network\n",
    "# print ('* Hidden Layer 1: {0}'.format(softmax_layer))\n",
    "\n",
    "\n",
    "# 1 cell LSTM\n",
    "cell = tf.nn.rnn_cell.LSTMCell(hidden_size, state_is_tuple=True) # A single LSTM cell\n",
    "\n",
    "# Multi LSTM cells\n",
    "rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cell] * n_layers)\n",
    "print ('* rnn_cells: {0}'.format(rnn_cells))\n",
    "outputs_T, states = tf.nn.dynamic_rnn(rnn_cells, X_rnn, dtype=tf.float32)\n",
    "print ('* output transpose: {0}'.format(outputs_T))\n",
    "\n",
    "# initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "# print ('* single cell: {0}'.format(cell))\n",
    "# ## Single rnn cell\n",
    "# outputs_T, states = tf.nn.dynamic_rnn(cell=cell, inputs=X_rnn, dtype=tf.float32)\n",
    "# print ('* outputs transpose: {0}'.format(outputs_T))\n",
    "\n",
    "outputs = tf.transpose(outputs_T, [1,0,2])\n",
    "print ('* outputs: {0}'.format(outputs))\n",
    "\n",
    "# Use output of last step as input for softmax layer\n",
    "# last_step = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)\n",
    "# print ('* last_step: {0}'.format(last_step))\n",
    "# input_softmax = tf.matmul(last_step, layer['weights']) + layer['biases']\n",
    "# print ('* input_softmax: {0}'.format(input_softmax))\n",
    "# y_rnn_softmax = tf.nn.softmax(input_softmax)\n",
    "# print ('* y_rnn_softmax: {0}'.format(y_rnn_softmax))\n",
    "\n",
    "\n",
    "## Use mean of output of all steps as input for softmax layer\n",
    "mean_step = tf.reduce_mean(input_tensor=outputs, axis=0)\n",
    "print ('* mean_step: {0}'.format(mean_step))\n",
    "\n",
    "# # Hidden layer 1:\n",
    "\n",
    "# hidden_1 = tf.nn.relu(tf.matmul(mean_step, hidden_1_layer['weights']) + hidden_1_layer['biases'])\n",
    "# print hidden_1\n",
    "# input_softmax = tf.matmul(hidden_1, softmax_layer['weights']) + softmax_layer['biases']\n",
    "\n",
    "input_softmax = tf.matmul(mean_step, layer['weights']) + layer['biases']\n",
    "print ('* input_softmax: {0}'.format(input_softmax))\n",
    "y_rnn_softmax = tf.nn.softmax(input_softmax)\n",
    "print ('* y_rnn_softmax: {0}'.format(y_rnn_softmax))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_3:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "BETA = 1e-3\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=input_softmax,labels=Y)\n",
    "print cross_entropy\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "print cost\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "\n",
    "minimize_cost = optimizer.minimize(cost)\n",
    "# actual_grads = [grad for grad, _ in optimizer.compute_gradients(cost,[layer['weights']])]\n",
    "# print actual_grads\n",
    "# num_grads = tf.gradients(cost, [layer['weights']])\n",
    "# print num_grads\n",
    "# errors = tf.contrib.losses.mean_squared_error(actual_grads[0], num_grads[0])\n",
    "y_true = tf.argmax(Y, dimension=1)\n",
    "y_pred = tf.argmax(y_rnn_softmax, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #slice data for testing model\n",
    "# X_train = X_train[:4000]\n",
    "# Y_train = Y_train[:4000]\n",
    "\n",
    "# print (X_train.shape)\n",
    "# print (Y_train.shape)\n",
    "# print (np.unique(np.argmax(Y_train, axis=1), return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classID = {\n",
    "    0: \"AirCon\",\n",
    "    1: \"Car horn\",\n",
    "    2: \"Children playing\",\n",
    "    3: \"Dog bark\",\n",
    "    4: \"Drilling\",\n",
    "    5: \"Engine idling\",\n",
    "    6: \"Gun shot\",\n",
    "    7: \"Jackhammer\",\n",
    "    8: \"Siren\",\n",
    "    9: \"Street music\"\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "TRAINING_EPOCHS = 1\n",
    "np.random.seed(2017)\n",
    "\n",
    "## Helper function for optimization\n",
    "def optimize(train_x, train_y, n_epochs, batch_size, session, saver):\n",
    "        n_samples = train_x.shape[0]\n",
    "        sample_IDs = np.arange(n_samples)\n",
    "        np.random.shuffle(sample_IDs)\n",
    "        train_x_p, train_y_p = train_x[sample_IDs], train_y[sample_IDs]\n",
    "        n_iterations = np.int(np.floor(n_samples/batch_size))+1\n",
    "        start_time = time.time()\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        print \"Training.......\"\n",
    "        print \"-- Elapsed time -- Epoch -- Cost value -- \"\n",
    "\n",
    "        for epoch in np.arange(n_epochs+1):\n",
    "            np.random.shuffle(sample_IDs)\n",
    "            train_x_p, train_y_p = train_x[sample_IDs], train_y[sample_IDs]\n",
    "            for itr in np.arange(n_iterations):\n",
    "                start = (itr * batch_size) % (n_samples - batch_size)\n",
    "                batch_x, batch_y = train_x_p[start:start + batch_size], train_y_p[start:start + batch_size]\n",
    "                feed_dict_train = {X: batch_x, Y: batch_y}\n",
    "                _, c = session.run([minimize_cost, cost], feed_dict=feed_dict_train)\n",
    "            \n",
    "                \n",
    "                \n",
    "#             if(epoch % (n_epochs/10) == 0):\n",
    "#                 print \"-- {:12.6f} -- {:5d} -- {:10.5f} -- {:15.11f} -- {:15.11f} --\".format((time.time() - start_time), \n",
    "#                                                                                     epoch, \n",
    "#                                                                                     c, \n",
    "#                                                                                     np.median(np.absolute(g[0])), \n",
    "#                                                                                     np.median(np.absolute(g[1])))\n",
    "            if(epoch % (n_epochs/10) == 0):\n",
    "                print \"-- {:12.6f} -- {:5d} -- {:10.5f} -- \".format((time.time() - start_time), \n",
    "                                                                                    epoch, \n",
    "                                                                                    c, \n",
    "                                                                                    )\n",
    "            cost_history = np.append(cost_history,c)\n",
    "#             Draw weights of convolutional layer\n",
    "#             if(epoch % (n_epochs/2) == 0):\n",
    "#                 plot_conv_weights(session, conv_weights[0], 'conv_1', 1, epoch)\n",
    "#                 plot_conv_weights(session, conv_weights[1], 'conv_2', 1, epoch)\n",
    "#                 plot_conv_weights(session, conv_weights[2], 'conv_3', 1, epoch)\n",
    "                  \n",
    "#         Save model in folder rnn_model\n",
    "        dir_path = './rnn_model/'\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "        saver.save(sess, 'rnn_model/new_cnn')\n",
    "        \n",
    "#         print running time and output cost value graph\n",
    "        print (\"---Running time: %s seconds ---\" % (time.time() - start_time))\n",
    "        print ('*'*50)\n",
    "        fig = plt.figure(figsize=(10,5))\n",
    "        plt.plot(cost_history)\n",
    "        plt.axis([0,epoch,0,np.max(cost_history)])\n",
    "        plt.show()\n",
    "\n",
    "def output_log_file(train, valid, test1, test2):\n",
    "    file_name = pickle_file.split('/')[-1].split('.')[0]\n",
    "    with open(\"log/logfile.txt\", \"ab\") as text_file:\n",
    "        text_file.write('='*60)\n",
    "        text_file.write('\\n')\n",
    "        text_file.write(\"Time: {0}\\n\".format(datetime.now()))\n",
    "        text_file.write(\"Data: \" + file_name +\"\\n\")    \n",
    "        \n",
    "        \n",
    "#         text_file.write(\"Number of input samples: {:6d}\\n\".format(N_SA)\n",
    "        text_file.write(\"Number of input features: {:5d}\\n\".format(IMG_FLAT_SIZE))\n",
    "#         text_file.write(\"Number of input labels: {:5d}\\n\".format(N_LABELS)\n",
    "        text_file.write(\"Number of convolutional layer: {0}\\n\".format(n_conv_layers))\n",
    "        text_file.write(\"\\tFilter size:\\t\")\n",
    "        for idx in np.arange(1, n_conv_layers + 1):\n",
    "            text_file.write('{0}\\t'.format(filter_size[idx]))\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"\\tNumber of filter:\\t\")\n",
    "        for idx in np.arange(1, n_conv_layers + 1):\n",
    "            text_file.write('{0}\\t'.format(n_filter[idx]))\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"Number of fully-connected layer: {0}\\n\".format(n_fc_layers))\n",
    "        text_file.write(\"Hidden units: {:3d}  - {:3d}\\n\".format(fc_size, N_LABELS))\n",
    "        text_file.write(\"Training epochs: {:5d}\\n\".format(TRAINING_EPOCHS))\n",
    "        text_file.write(\"Batch size: {:3d}\\n\".format(BATCH_SIZE))\n",
    "                        \n",
    "        text_file.write('\\tTrain\\tValid\\tTest1\\tYoutube\\n')\n",
    "        text_file.write(\"Fscore\\t {:1.2f} \\t {:1.2f} \\t {:1.2f} \\t {:1.2f} \\n\".format(train[0], \n",
    "                                                                                      valid[0], \n",
    "                                                                                      test1[0], \n",
    "                                                                                      test2[0]))\n",
    "        text_file.write(\"Acc\\t {:1.2f} \\t {:1.2f} \\t {:1.2f} \\t {:1.2f} \\n\".format(train[1], \n",
    "                                                                                   valid[1], \n",
    "                                                                                   test1[1], \n",
    "                                                                                   test2[1]))\n",
    "        \n",
    "        \n",
    "\n",
    "## Helper function to print confusion matrix\n",
    "def make_prediction(test_x, test_y, session, batch_size):\n",
    "    print \"Making prediction.......\"\n",
    "    start_time = time.time()\n",
    "    n_samples = test_x.shape[0]\n",
    "    sample_IDs = np.arange(n_samples)\n",
    "    n_iterations = np.int(np.floor(n_samples/batch_size))+1\n",
    "    pred = np.zeros(n_samples)\n",
    "    true = np.zeros(n_samples)\n",
    "    for itr in np.arange(n_iterations):\n",
    "        start = (itr * batch_size) % (n_samples - batch_size)\n",
    "        batch_x, batch_y = test_x[start:start + batch_size], test_y[start:start + batch_size]\n",
    "        feed_dict_test = {X_cnn: batch_x, Y: batch_y}\n",
    "        pred[start:start + batch_size], true[start:start + batch_size] = session.run([y_pred, y_true], feed_dict=feed_dict_test)\n",
    "\n",
    "## Performance Evaluation metrics\n",
    "    ## Accuracy\n",
    "    accuracy = accuracy_score(true, pred)\n",
    "    print (\"Accuracy: {:3.2f}\".format(accuracy))\n",
    "    ## F-score\n",
    "    if(N_LABELS == 2):\n",
    "        p,r,f,s = precision_recall_fscore_support(true, pred, average='binary')\n",
    "    else:\n",
    "        p,r,f,s = precision_recall_fscore_support(true, pred, average='macro')\n",
    "    print (\"F-Score: {:3.2f}\".format(f))\n",
    "    \n",
    "    ## Confusion matrix\n",
    "    print (\"Confusion Matrix\")\n",
    "    print confusion_matrix(true, pred)\n",
    "#     print (\"Analyzing result\")\n",
    "#     for ID, name in classID.items():\n",
    "#         x = pred[(true == ID)*(pred != ID)]  \n",
    "#         unique_class, unique_count = np.unique(x, return_counts=True)\n",
    "#         print ('-'*50)\n",
    "#         print \" Class {0} is mistaken with: \".format(name)\n",
    "#         for c, y in zip(unique_class, unique_count):        \n",
    "#             print \"# {0}: {1} samples\".format(classID[c], y)\n",
    "#     print ('-'*50)\n",
    "    print (\"---Running time: {0} seconds ---\".format((time.time() - start_time)))\n",
    "    print ('*'*50)\n",
    "    return accuracy, f\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.......\n",
      "-- Elapsed time -- Epoch -- Cost value -- \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-766e7968b0ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAINING_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     print (\"Making prediction on training set\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-c5d933bcd6d9>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(train_x, train_y, n_epochs, batch_size, session, saver)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_x_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mfeed_dict_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mminimize_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kyle/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kyle/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kyle/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/kyle/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kyle/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    optimize(X_train, Y_train, TRAINING_EPOCHS, batch_size, sess, saver)\n",
    "    \n",
    "    print (\"Making prediction on training set\")\n",
    "    train = make_prediction(X_train, Y_train, sess, batch_size)\n",
    "    print (\"Making prediction on validation set\")\n",
    "    valid = make_prediction(X_valid, Y_valid, sess, batch_size)\n",
    "    print (\"Making training prediction on test 1 set\")\n",
    "    test1 = make_prediction(X_test1, Y_test1, sess, batch_size)\n",
    "    print (\"Making training prediction on test 2 set\")\n",
    "    test2 = make_prediction(X_test2, Y_test2, sess, batch_size)\n",
    "#     output_log_file(train, valid, test1, test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
