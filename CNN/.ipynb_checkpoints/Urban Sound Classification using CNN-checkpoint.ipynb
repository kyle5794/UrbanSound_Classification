{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import specgram\n",
    "import time\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 9\n",
    "plt.rcParams['ytick.labelsize'] = 9\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "plt.rcParams['figure.titlesize'] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def windows(data, window_size):\n",
    "    start = 0\n",
    "    while start < len(data):\n",
    "        yield start, start + window_size\n",
    "        start += (window_size / 2)\n",
    "\n",
    "def extract_features(parent_dir,sub_dirs,file_ext=\"*.wav\",bands = 128, frames = 128):\n",
    "    window_size = 512 * (frames - 1)\n",
    "    log_specgrams = []\n",
    "    labels = []\n",
    "    namePath = np.empty(0)\n",
    "    for l, sub_dir in enumerate(sub_dirs):\n",
    "        for fn in glob.glob(os.path.join(parent_dir, sub_dir, file_ext)):\n",
    "            \n",
    "            sound_clip,sr = librosa.load(fn)\n",
    "            label = fn.split('/')[-1].split('-')[1]\n",
    "            for (start,end) in windows(sound_clip,window_size):\n",
    "                if(len(sound_clip[start:end]) == window_size):\n",
    "                    namePath = np.append(namePath, fn)\n",
    "                    signal = sound_clip[start:end]\n",
    "                    melspec = librosa.feature.melspectrogram(signal, sr=sr, n_fft=512, hop_length=512)\n",
    "                    logspec = librosa.logamplitude(melspec)\n",
    "                    logspec = logspec.T.flatten()[:, np.newaxis].T\n",
    "                    log_specgrams.append(logspec)\n",
    "                    labels.append(label)\n",
    "            \n",
    "#     log_specgrams = np.asarray(log_specgrams).reshape(len(log_specgrams),bands,frames,1)\n",
    "#     features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams))), axis = 3)\n",
    "#     for i in range(len(features)):\n",
    "#         features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "    features = np.asarray(log_specgrams).reshape(len(log_specgrams),bands,frames)\n",
    "    \n",
    "    return np.array(features), np.array(labels,dtype = np.int), np.array(namePath, dtype=np.string_)\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Extracting features.... ---\n",
      "---Loading time: 1862.92143703 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# # Only for the first time running\n",
    "# parent_dir = '../../data/UrbanSound8K/audio/'\n",
    "# sub_dirs= ['fold1','fold2', 'fold3', 'fold4', 'fold5', 'fold6', 'fold7', 'fold8','fold9', 'fold10']\n",
    "# # sub_dirs= ['fold1']\n",
    "# print \"---Extracting features.... ---\"\n",
    "# start_time = time.time()\n",
    "# features,labels, files_name = extract_features(parent_dir,sub_dirs)\n",
    "# print \"---Loading time: {0} seconds ---\".format(time.time() - start_time)\n",
    "# one_hot_labels = one_hot_encode(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Fold 1 2 3 4 5\n",
    "# np.save(\"cnn_features_fold_12345\", features, allow_pickle=True)\n",
    "# np.save(\"cnn_labels_fold_12345\", labels, allow_pickle=True)\n",
    "# np.save(\"cnn_file_names_fold_12345\", files_name, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2da63ccfed78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# All Folds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cnn_features_full\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cnn_labels_full\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cnn_file_names_full\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "# # All Folds\n",
    "# np.save(\"cnn_features_full\", features, allow_pickle=True)\n",
    "# np.save(\"cnn_labels_full\", labels, allow_pickle=True)\n",
    "# np.save(\"cnn_file_names_full\", files_name, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = np.load(\"cnn_features_full.npy\", allow_pickle=True)\n",
    "labels = np.load(\"cnn_labels_full.npy\", allow_pickle=True)\n",
    "file_names = np.load(\"cnn_file_names_full.npy\", allow_pickle=True)\n",
    "one_hot_labels = one_hot_encode(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7478, 128, 128)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = features[:,:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7478, 128, 128, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bands = 128\n",
    "frames = 128\n",
    "feature_size = bands*frames\n",
    "specgram_shape = (bands, frames)\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def new_biases(length):\n",
    "    initial = tf.constant(1.0, shape = [length])\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   use_pooling=True):  # Use 4x2 max-pooling.\n",
    "\n",
    "    # Shape of the filter-weights for the convolution.\n",
    "    # This format is determined by the TensorFlow API.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "#     shape = [filter_size, filter_size, num_filters]\n",
    "\n",
    "    # Create new weights aka. filters with the given shape.\n",
    "    weights = new_weights(shape=shape)\n",
    "\n",
    "    # Create new biases, one for each filter.\n",
    "    biases = new_biases(length=num_filters)\n",
    "\n",
    "    # Create the TensorFlow operation for convolution.\n",
    "    # Note the strides are set to 1 in all dimensions.\n",
    "    # The first and last stride must always be 1,\n",
    "    # because the first is for the image-number and\n",
    "    # the last is for the input-channel.\n",
    "    # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "    # is moved 2 pixels across the x- and y-axis of the image.\n",
    "    # The padding is set to 'SAME' which means the input image\n",
    "    # is padded with zeroes so the size of the output is the same.\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='VALID')\n",
    "\n",
    "    # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    layer += biases\n",
    "\n",
    "    # Use pooling to down-sample the image resolution?\n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling, which means that we\n",
    "        # consider 2x2 windows and select the largest value\n",
    "        # in each window. Then we move 2 pixels to the next window.\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 4, 2, 1],\n",
    "                               strides=[1, 4, 2, 1],\n",
    "                               padding='VALID')\n",
    "\n",
    "    # Rectified Linear Unit (ReLU).\n",
    "    # It calculates max(x, 0) for each input pixel x.\n",
    "    # This adds some non-linearity to the formula and allows us\n",
    "    # to learn more complicated functions.\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Note that ReLU is normally executed before the pooling,\n",
    "    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "    # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "    # We return both the resulting layer and the filter-weights\n",
    "    # because we will plot the weights later.\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def conv2d(x, W):\n",
    "#     return tf.nn.conv2d(x,W,strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "# def apply_convolution(x,kernel_size,num_channels,depth):\n",
    "#     weights = weight_variable([kernel_size, kernel_size, num_channels, depth])\n",
    "#     biases = bias_variable([depth])\n",
    "#     return tf.nn.relu(tf.add(conv2d(x, weights),biases))\n",
    "\n",
    "# def apply_max_pool(x,kernel_size,stride_size):\n",
    "#     return tf.nn.max_pool(x, ksize=[1, kernel_size, kernel_size, 1], \n",
    "#                           strides=[1, stride_size, stride_size, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 activation=\"\"): # Use Rectified Linear Unit (ReLU)?\n",
    "                 \n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    activation = activation.lower()\n",
    "    if (activation ==\"sigmoid\"):\n",
    "        layer = tf.nn.sigmoid(layer)\n",
    "    elif (activation ==\"relu\"):\n",
    "        layer = tf.nn.relu(layer)    \n",
    "    return layer, weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convolution layer 1 params\n",
    "filter_l1_size = 5\n",
    "n_filter_l1 = 24\n",
    "stride_l1 = [1, 4, 2, 1]\n",
    "activation_func_l1 = \"relu\"\n",
    "\n",
    "#Convolution layer 2 params\n",
    "filter_l2_size = 5\n",
    "n_filter_l2 = 48\n",
    "stride_l2 = [1, 4, 2, 1]\n",
    "activation_func_l2 = \"relu\"\n",
    "\n",
    "#Convolution layer 3 params\n",
    "filter_l3_size = 5\n",
    "n_filter_l3 = 48\n",
    "activation_func_l3 = \"relu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fully connected layer 4\n",
    "\n",
    "num_hidden = 64\n",
    "activation_func_l4 = \"relu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, bands, frames, 1])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, num_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_conv1, weights_1 = new_conv_layer(input=X, \n",
    "                                        num_input_channels=1, \n",
    "                                        filter_size=filter_l1_size, \n",
    "                                        num_filters=n_filter_l1, \n",
    "                                        use_pooling=True)\n",
    "\n",
    "layer_conv2, weights_2 = new_conv_layer(input=layer_conv1, \n",
    "                                        num_input_channels=n_filter_l1, \n",
    "                                        filter_size=filter_l2_size, \n",
    "                                        num_filters=n_filter_l2, \n",
    "                                        use_pooling=True)\n",
    "\n",
    "layer_conv3, weights_3 = new_conv_layer(input=layer_conv2, \n",
    "                                        num_input_channels=n_filter_l2, \n",
    "                                        filter_size=filter_l3_size, \n",
    "                                        num_filters=n_filter_l3, \n",
    "                                        use_pooling=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_flat, num_features = flatten_layer(layer_conv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_fc1, weights_4, biases_4 = new_fc_layer(layer_flat, num_inputs=num_features, num_outputs=num_hidden, activation=\"relu\")\n",
    "layer_fc1_drop = tf.nn.dropout(layer_fc1, keep_prob=keep_prob)\n",
    "layer_fc2, weights_5, biases_5 = new_fc_layer(layer_fc1, num_inputs=num_hidden, num_outputs=num_labels)\n",
    "layer_fc2_drop = tf.nn.dropout(layer_fc2, keep_prob=keep_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.nn.softmax(layer_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(?, 128, 128, 1) dtype=float32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu:0' shape=(?, 31, 62, 24) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_1:0' shape=(?, 6, 29, 48) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_2:0' shape=(?, 2, 25, 48) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(?, 2400) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_3:0' shape=(?, 64) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_4:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X = tf.placeholder(tf.float32, shape=[None,bands,frames,num_channels])\n",
    "# Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "# cov = apply_convolution(X,kernel_size,num_channels,depth)\n",
    "\n",
    "# shape = cov.get_shape().as_list()\n",
    "# cov_flat = tf.reshape(cov, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "# f_weights = weight_variable([shape[1] * shape[2] * depth, num_hidden])\n",
    "# f_biases = bias_variable([num_hidden])\n",
    "# f = tf.nn.sigmoid(tf.add(tf.matmul(cov_flat, f_weights),f_biases))\n",
    "\n",
    "# out_weights = weight_variable([num_hidden, num_labels])\n",
    "# out_biases = bias_variable([num_labels])\n",
    "# y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cross_entropy = -tf.reduce_sum(Y * tf.log(y_))\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "# correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,labels=Y)\n",
    "cost = tf.reduce_mean(cross_entropy + \n",
    "                      beta*tf.nn.l2_loss(weights_4) + \n",
    "                      beta*tf.nn.l2_loss(biases_4) + \n",
    "                     beta*tf.nn.l2_loss(weights_5) +\n",
    "                     beta*tf.nn.l2_loss(biases_5))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(features, one_hot_labels, test_size=0.3, random_state=5, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = train_x[[5, 7, 9, 40, 45]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 128, 128, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = np.arange(np.shape(train_x)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  91, 3896, 4831, ...,  274, 4759, 2099])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.......\n",
      "------ Elapsed time -------  Iter ---- Cost \n",
      "------    1.2265270 -------     0 ---- 135.6324310303 \n",
      "------   10.9214661 -------    10 ---- 3.2905840874 \n",
      "------   20.4728220 -------    20 ---- 2.7967858315 \n",
      "------   30.0130320 -------    30 ---- 2.7790186405 \n",
      "------   39.6735761 -------    40 ---- 2.6604037285 \n",
      "------   49.3095810 -------    50 ---- 2.7010796070 \n",
      "------   50.6744919 -------    60 ----          nan \n",
      "------   50.6940291 -------    70 ----          nan \n",
      "------   50.7137630 -------    80 ----          nan \n",
      "------   50.7345750 -------    90 ----          nan \n",
      "---Training time: 57.0710060596 seconds ---\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABOEAAAM4CAYAAACKs/aCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzs3U9o1Xf+7/G3TUxiG2smJrFEpSpUWrDdKAxjAnY1i0K7\nkK5GRTdTBGFkdCMDdjrboQpaihRGsEpxUygzXXU5YjqLym8hyAz8gkrRQ2vinxo1IU3IXdzbcHO1\ntup51cP9PR678/18zjff74H35sk53yyam5ubKwAAAAAg5pmnfQEAAAAA8P87EQ4AAAAAwkQ4AAAA\nAAgT4QAAAAAgTIQDAAAAgDARDgAAAADCRDgAAAAACBPhAAAAACBMhAMAAACAMBHu/zh79uzTvgTg\nIcwotC7zCa3NjELrMp/Q2po9o02NcOPj4/XnP/+5tm3bVn/605/q6tWr9+25e/du/fWvf63t27fX\nvn376j//+c/PWvvBiRMn6o9//OPPPufPNTIy8sjvAX45ZhRal/mE1mZGoXWZT2htzZ7Rpka4Y8eO\nVW9vbx09erTWrVtXR44cuW/PqVOnanJysg4fPlzDw8N16NChmpmZ+cm1ixcv1l/+8pf65z//+Ujn\nBAAAAICnrWkRbmxsrC5cuFA7d+6s5cuX144dO+rKlSt18eLF+T3T09M1MjJS27Ztq4GBgdq6dWu1\ntbXVuXPnHrr2w/l/85vf1I4dOxb83Z96HwAAAAA8bU2LcI1Go5YtW1Y9PT1VVdXZ2VmDg4MLfpI6\nPj5e09PTtWbNmvlja9eurUaj8dC1qqpf//rX9dvf/raeeWbhJf/U+wAAAADgaWtahJuYmKiurq4F\nx5YsWVITExPzr+/cuVNtbW3V3t6+YM/t27cfuvYwj/u+/9crr7zySPuBX9aKFSue9iUAP8J8Qmsz\no9C6zCe0tma3ovaf3vLzLF26tKamphYcm5ycrKVLl86/7u7urtnZ2ZqZmZmPZpOTk7Vq1aqHrj3M\no77v7Nmz9z1Y75VXXqm33nrr0W4Y+EXt2rXraV8C8CPMJ7Q2Mwqty3xCa3vrrbfqH//4R/373/9e\ncHxoaKiGh4cf+XxNi3CDg4P13Xff1a1bt6qnp6empqaq0WjUypUr5/f09fVVZ2dnXbp0qV566aWq\nqrp8+XK9/vrr1dfXVx0dHfetbdmy5aF/98fO+WPvGx4e/tEP6sZ//7tme/oe+d6BvOeff/6Rv+EK\n/DLMJ7Q2Mwqty3xC62pvb69f/epX9dZbbzXti1tNi3D9/f21YcOGOnnyZP3ud7+rv//977V69eoa\nGBiogwcP1p49e+qFF16ozZs31+nTp2v37t01MjJSs7OztXHjxmpvb6+hoaH71jZt2vTQv9vR0fHA\nc/7U+x5kZnKyZp77/nE/AiBobm6uvv/efEIrMp/Q2swotC7zCf+zNO2ZcFVVu3fvruvXr9fevXtr\ndHS09u7dW9PT09VoNOrevXtVVbV9+/bq6uqqffv21ZkzZ2r//v3zPyN92NrDPO77AAAAAOCXsGhu\nbm7uaV9Eq7h2/r9qpu+Fp30ZwAP09vbWjRs3nvZlAA9gPqG1mVFoXeYTWtfixYurv7+/qeds6jfh\nAAAAAID7iXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABh\nIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAA\nYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAA\nAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAA\nAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcA\nAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgH\nAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgI\nBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECY\nCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABA\nmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAA\nQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAA\nAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEA\nAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIB\nAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbC\nAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAm\nwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQ\nJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAA\nECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAA\nABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAA\nAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAA\nAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlw\nAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJ\ncAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACE\niXAAAAAAECbCAQAAAECYCAcAAAAAYe3NPNn4+Hh98MEHNTo6Wi+++GLt2bOnVq5cuWDP3bt368MP\nP6zz58/XwMBAvfPOO/Xyyy8/0dpXX31V77///vzf6OnpqY8++qiZtwYAAAAAj62p34Q7duxY9fb2\n1tGjR2vdunV15MiR+/acOnWqJicn6/DhwzU8PFyHDh2qmZmZJ1qbnZ2t1157rY4fP17Hjx+vw4cP\nN/O2AAAAAOCJNC3CjY2N1YULF2rnzp21fPny2rFjR125cqUuXrw4v2d6erpGRkZq27ZtNTAwUFu3\nbq22trY6d+7cY69VVd27d696enqqu7u7uru767nnnmvWbQEAAADAE2tahGs0GrVs2bLq6empqqrO\nzs4aHBysq1evzu8ZHx+v6enpWrNmzfyxtWvXVqPReOy1qqqbN29Wo9Gozz//vEZHR5t1SwAAAADQ\nFE17JtzExER1dXUtOLZkyZKamJiYf33nzp1qa2ur9vb2BXtu37792GtVVa+++motWbKkvv766/r0\n00/rzTffrLfffvuB13n27NkaGRlZcGzFihW1a9eueq67u9p6ex//QwBiFi9eXL3mE1qS+YTWZkah\ndZlPaF2LFi2qqqoTJ07Ut99+u2BtaGiohoeHH/mcTYtwS5curampqQXHJicna+nSpfOvu7u7a3Z2\ntmZmZuaD2uTkZK1ateqx16qq1q9fX+vXr6+qqi1bttR7771Xb7zxRj377LP3Xefw8PCPflB379yp\nmRs3nvCTABJ6e3vrhvmElmQ+obWZUWhd5hNa1+LFi6u/v7927drVtHM27eeog4OD9d1339WtW7eq\nqmpqaqoajcaC/47a19dXnZ2ddenSpfljly9frpUrV1ZfX191dHQ80trg4OB91/HDT1Z/uA4AAAAA\neNqaFuH6+/trw4YNdfLkyRofH69PPvmkVq9eXQMDA3Xw4MH65ptvqqOjozZv3lynT5+ua9eu1Wef\nfVazs7O1cePG6ujoqKGhoUda27RpU1VVffnll9VoNOrq1av18ccfV39/f73wwgvNujUAAAAAeCJN\ni3BVVbt3767r16/X3r17a3R0tPbu3VvT09PVaDTq3r17VVW1ffv26urqqn379tWZM2dq//798z8x\nfZy16enp+uKLL+rAgQP17rvv1s2bN+vAgQP1zDNNvTUAAAAAeGyL5ubm5p72RbSKa+f/q2b6fIMO\nWpHnZUDrMp/Q2swotC7zCa3rh2fCNZOviwEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAA\nAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEA\nAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIB\nAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbC\nAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAm\nwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQ\nJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAA\nECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAA\nABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAA\nAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAA\nAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlw\nAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJ\ncAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACE\niXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAA\nhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAA\nAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAA\nAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwA\nAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIc\nAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEi\nHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABh\nIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAA\nYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAA\nAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAA\nAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAENbezJONj4/XBx98\nUKOjo/Xiiy/Wnj17auXKlQv23L17tz788MM6f/58DQwM1DvvvFMvv/xybA0AAAAAnramfhPu2LFj\n1dvbW0ePHq1169bVkSNH7ttz6tSpmpycrMOHD9fw8HAdOnSoZmZmYmsAAAAA8LQ1LcKNjY3VhQsX\naufOnbV8+fLasWNHXblypS5evDi/Z3p6ukZGRmrbtm01MDBQW7durba2tjp37lxkDQAAAABaQdMi\nXKPRqGXLllVPT09VVXV2dtbg4GBdvXp1fs/4+HhNT0/XmjVr5o+tXbu2Go1GZA0AAAAAWkHTngk3\nMTFRXV1dC44tWbKkJiYm5l/fuXOn2traqr29fcGe27dvR9Ye5OzZszUyMrLg2IoVK2rXrl31XHd3\ntfX2Pt4HAEQtXry4es0ntCTzCa3NjELrMp/QuhYtWlRVVSdOnKhvv/12wdrQ0FANDw8/8jmbFuGW\nLl1aU1NTC45NTk7W0qVL5193d3fX7OxszczMzEezycnJWrVqVWTtQYaHh3/0g7rb+VzN3LjxZB8E\nENHb21s3zCe0JPMJrc2MQusyn9C6Fi9eXP39/bVr166mnbNpP0cdHBys7777rm7dulVVVVNTU9Vo\nNBb8d9S+vr7q7OysS5cuzR+7fPlyrVy5svr6+qqjo6Npa4ODg498D4s6Oh75PQAAAADwU5oW4fr7\n+2vDhg118uTJGh8fr08++aRWr15dAwMDdfDgwfrmm2+qo6OjNm/eXKdPn65r167VZ599VrOzs7Vx\n48bq6OiooaGhpq1t2rSpWbcGAAAAAE+kaRGuqmr37t11/fr12rt3b42OjtbevXtrenq6Go1G3bt3\nr6qqtm/fXl1dXbVv3746c+ZM7d+/f/5npIk1AAAAAHjaFs3Nzc097YtoFWNjY/X9998/7csAHsDz\nMqB1mU9obWYUWpf5hNb1wzPhmqmp34QDAAAAAO4nwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAA\nECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAA\nABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAA\nAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAA\nAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlw\nAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJ\ncAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACE\niXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAA\nhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAA\nAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAA\nAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwA\nAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIc\nAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEi\nHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABh\nIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAA\nYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAA\nAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAA\nAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcA\nAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgH\nAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgI\nBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECY\nCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAAQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABA\nmAgHAAAAAGEiHAAAAACEiXAAAAAAECbCAQAAAECYCAcAAAAAYSIcAAAAAISJcAAAAAAQJsIBAAAA\nQJgIBwAAAABhIhwAAAAAhIlwAAAAABAmwgEAAABAmAgHAAAAAGEiHAAAAACEtTfrROPj4/XBBx/U\n6Ohovfjii7Vnz55auXLlffvu3r1bH374YZ0/f74GBgbqnXfeqZdffvmJ1r766qt6//335/9GT09P\nffTRR826NQAAAAB4Ik37JtyxY8eqt7e3jh49WuvWrasjR448cN+pU6dqcnKyDh8+XMPDw3Xo0KGa\nmZl5orXZ2dl67bXX6vjx43X8+PE6fPhws24LAAAAAJ5YUyLc2NhYXbhwoXbu3FnLly+vHTt21JUr\nV+rixYsL9k1PT9fIyEht27atBgYGauvWrdXW1lbnzp177LWqqnv37lVPT091d3dXd3d3Pffcc824\nLQAAAABoiqZEuEajUcuWLauenp6qqurs7KzBwcG6evXqgn3j4+M1PT1da9asmT+2du3aajQaj71W\nVXXz5s1qNBr1+eef1+joaDNuCQAAAACa5pGeCTc3Nzf/E9D/28TERHV1dS04tmTJkpqYmFhw7M6d\nO9XW1lbt7e0L9t2+ffux16qqXn311VqyZEl9/fXX9emnn9abb75Zb7/99gPv4ezZszUyMrLg2IoV\nK2rXrl31/PPP19zc3M/8NIBf0uLFi6u3t/dpXwbwAOYTWpsZhdZlPqF1LVq0qKqqTpw4Ud9+++2C\ntaGhoRoeHn7kcz5ShPvXv/71wGe9/f73v6+pqakFxyYnJ2vp0qULjnV3d9fs7GzNzMzMB7XJycla\ntWrVY69VVa1fv77Wr19fVVVbtmyp9957r95444169tln77vW4eHhH/2gbt++Xd9///2jfCTAL6S3\nt7du3LjxtC8DeADzCa3NjELrMp/QuhYvXlz9/f21a9eupp3zkSLc5s2ba/PmzfcdHxsbq7/97W91\n69at6unpqampqWo0Gvf9d9S+vr7q7OysS5cu1UsvvVRVVZcvX67XX3+9+vr6qqOj45HWtmzZct+1\n/PCT1Vu3bj0wwgEAAADAL60pz4Tr7++vDRs21MmTJ2t8fLw++eSTWr16da1bt67u3LlTBw8erG++\n+aY6Ojpq8+bNdfr06bp27Vp99tlnNTs7Wxs3bqyOjo4aGhp6pLVNmzZVVdWXX35ZjUajrl69Wh9/\n/HH19/fXCy+80IxbAwAAAIAn1pQIV1W1e/fuun79eu3du7dGR0frD3/4Q1X97/+I2mg06t69e1VV\ntX379urq6qp9+/bVmTNnav/+/fM/MX2ctenp6friiy/qwIED9e6779bNmzfrwIED9cwzTbs1AAAA\nAHgii+b8J4J5Y2NjngkHLcrzMqB1mU9obWYUWpf5hNb1wzPhmsnXxQAAAAAgTIQDAAAAgDARDgAA\nAADCRDgAAAAACBPhAAAAACBMhAMAAACAMBEOAAAAAMJEOAAAAAAIE+EAAAAAIEyEAwAAAIAwEQ4A\nAAAAwkQ4AAAAAAgT4QAAAAAgTIQDAAAAgDARDgAAAADCRDgAAAAACBPhAAAAACBMhAMAAACAMBEO\nAAAAAMJEOAAAAAAIE+EAAAAAIEyEAwAAAIAwEQ4AAAAAwkQ4AAAAAAgT4QAAAAAgTIQDAAAAgDAR\nDgAAAADCRDgAAAAACBPhAAAAACBMhAMAAACAMBEOAAAAAMJEOAAAAAAIE+EAAAAAIEyEAwAAAIAw\nEQ4AAAAAwkQ4AAAAAAgT4QAAAAAgTIQDAAAAgDARDgAAAADCRDgAAAAACBPhAAAAACBMhAMAAACA\nMBEOAAAAAMJEOAAAAAAIE+EAAAAAIEyEAwAAAIAwEQ4AAAAAwkQ4AAAAAAgT4QAAAAAgTIQDAAAA\ngDARDgAAAADCRDgAAAAACBPhAAAAACBMhAMAAACAMBEOAAAAAMJEOAAAAAAIE+EAAAAAIEyEAwAA\nAIAwEQ4AAAAAwkQ4AAAAAAgT4QAAAAAgTIQDAAAAgDARDgAAAADCRDgAAAAACBPhAAAAACBMhAMA\nAACAMBFwbdOmAAANK0lEQVQOAAAAAMJEOAAAAAAIE+EAAAAAIEyEAwAAAIAwEQ4AAAAAwkQ4AAAA\nAAgT4QAAAAAgTIQDAAAAgDARDgAAAADCRDgAAAAACBPhAAAAACBMhAMAAPhf7d1raNZ1/8Dxzzxt\nq4nTbYYztZZFhhFUKLmFQiYEZaRWD0rUTkqKRYGxwtQHQUIaaiZEgdpBSKgh9aQHkalFRzo4U5sH\nSOdhay6n2xyb+z+4Uf671ZU3+97XdbPX65nf33X9ru8GH37X3l4HAEhMhAMAAACAxEQ4AAAAAEhM\nhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAA\nAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4\nAAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACA\nxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMA\nAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhM\nhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAA\nAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4\nAAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACA\nxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMA\nAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhM\nhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAA\nAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4\nAAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACA\nxEQ4AAAAAEhMhAMAAACAxEQ4AAAAAEhMhAMAAACAxPr11Inq6+tjzZo1UVNTE6NGjYr58+fH8OHD\nL7jd6dOnY+3atfHLL7/E0KFD46mnnoobb7zxb4+ds379+vj555/j9ddf/0fnBAAAAIBM67FXwq1b\nty6GDBkSq1evjrKysli1atVFb/fuu+9GS0tLrFy5MioqKmLFihXR3t7+t8f2798fy5Yti61bt17W\nOQEAAAAg03okwtXV1UV1dXXMmjUrioqKYubMmXHo0KHYv39/l9u1tbXFjh074pFHHomhQ4fGtGnT\nom/fvvH99993e+zcY9xxxx0xc+bMf3xOAAAAAMgGPRLhamtrY9CgQVFYWBgREbm5uVFaWhqHDx/u\ncrv6+vpoa2uLa6655vzatddeG7W1td0ei4gYP358TJkyJfr06fOPzwkAAAAA2eCyPhOus7Pzom/z\nbGpqiry8vC5r+fn50dTU1GXt1KlT0bdv3+jXr1+X2508ebLbY935T+93Mf//HEB2ycnJif79+2d6\nG8BFmE/IbmYUspf5hOyVohFd1hm//vrri37W25NPPhmtra1d1lpaWmLgwIFd1goKCqKjoyPa29vP\n/zAtLS1x9dVXd3usO5d7v+3bt8eOHTu6rI0ZMyamTp0agwcP7vaxgMwqKSnJ9BaASzCfkN3MKGQv\n8wnZbcuWLfHbb791WSsvL4+KiorLPtdlRbgJEybEhAkTLlivq6uLt99+OxobG6OwsDBaW1ujtrb2\ngm9HLS4ujtzc3Dhw4EBcf/31ERFx8ODBmDRpUhQXF8eAAQMuODZx4sRu93Spc17qfhUVFRf9RW3Z\nsiWmTp36978EICPWr18fs2fPzvQ2gIswn5DdzChkL/MJ2e1cK+qpXtQjnwlXUlISY8eOjY0bN0Z9\nfX28//77MWLEiCgrK4tTp07F4sWL4+jRozFgwICYMGFCbNq0KY4fPx4ff/xxdHR0xG233RYDBgyI\n8vLyC47dfvvt3T72pc75d/f7d/9eNYHscuzYsUxvAbgE8wnZzYxC9jKfkN16uhX1SISLiJg3b178\n+eef8cwzz0RNTU0sXLgwIv717aW1tbXR3NwcERGPPvpo5OXlxXPPPRdffvllPP/88+ffRtrdse78\np/cDAAAAgP+GHitVxcXFsWzZsgvWhwwZEu+88875fxcUFMSiRYsueo7ujp0zadKkmDRp0mXfDwAA\nAAAypcdeCQcAAAAAXFzfpUuXLs30JrLFyJEjM70FoBtmFLKX+YTsZkYhe5lPyG49OaM5nZ2dnT12\nNgAAAADgAt6OCgAAAACJiXAAAAAAkJgIBwAAAACJiXAAAAAAkJgIBwAAAACJ9cv0BjKtvr4+1qxZ\nEzU1NTFq1KiYP39+DB8+PNPbgl7p7Nmz8eGHH8YXX3wRLS0tMWbMmJg7d24MHjzYrEKWaWhoiMrK\nyrj77rtjxowZZhSyRFNTU1RVVcV3330XJSUlsXjxYvMJWWL37t2xYcOGOHz4cIwYMSLmzJkTo0eP\nNqOQIe3t7bF69eo4c+ZMVFZWRkT3jagnZrXXvxJu3bp1MWTIkFi9enWUlZXFqlWrMr0l6LWam5uj\nvr4+Fi1aFK+++mqcPn063nvvvYgwq5BNzp49GytWrOiyZkYh8xobG6OysjKam5tjwYIFsXDhwogw\nn5ANWltbY/ny5VFRURErV66Mm266KV577bXo7Ow0o5ABO3fujBdffDF+/fXXLuvdzWNPzGqvjnB1\ndXVRXV0ds2bNiqKiopg5c2YcOnQo9u/fn+mtQa9UUFAQCxYsiLKyshg2bFiUl5fHkSNHoq6uLnbu\n3GlWIUtUVVXF8OHD45ZbbomIMKOQJTZs2BDjxo2LuXPnxg033BCDBg3yfBeyxPHjx6O5uTnuuuuu\nKC4ujsmTJ8eJEyfi4MGDrqGQAUePHo37778/7rnnnvNrx48fv+Q1s6ee7/bqCFdbWxuDBg2KwsLC\niIjIzc2N0tLSOHz4cIZ3BkRE/PTTT3HzzTdHbW1tFBYWmlXIAseOHYvPP/88Zs+efX7NjELmnTlz\nJr755pv4/fffY968efHCCy9EdXW157uQJUpLS+Oqq66KjRs3xunTp6OqqirGjx8fJ0+edA2FDJg8\neXKUl5dHTk7O+bUjR45c8prZU893e3WEa2pqiry8vC5r+fn50dTUlKEdAed89NFH0dDQENOmTTOr\nkEU++OCDmD59elxxxRXn18woZN6RI0eis7MzHnjggXj55Zdj/PjxsXz58mhsbDSfkAX69esXc+bM\niW3btsWzzz4b27Zti3vvvdc1FLJId/PYU7PaqyPcwIEDo7W1tctaS0tLDBw4MEM7AiIiPvnkk9i6\ndWu89NJLkZuba1YhS5x7yf3EiRO7rJtRyLz29vaIiLj11lujtLQ0pk2bFvn5+dG/f3/zCVmgqakp\n3njjjViyZEm89dZb8fDDD8crr7wSHR0dZhSyRHfPaXvq+W6vjnClpaXx119/RWNjY0T868Mya2tr\nfRMNZNCPP/4Yn376aSxZsiQGDRoUEWYVssW3334bDQ0N8fjjj8ecOXNi+/btUVVVFQcOHDCjkGFF\nRUVx9uzZqKurO7/W0dERzc3N5hOyQHV1deTl5cXo0aMjJycn7rvvvujTp0+0tLSYUcgS3f3d2VN/\nk/ZdunTp0p7e+P+KK6+8Mnbv3h27d++O6667LjZv3hzt7e3x0EMPZXpr0GstX748pk+fHiNGjIi2\ntrZoa2uLwsLC2LNnj1mFDBs1alRMnjw5pkyZElOmTIm6uroYO3ZsTJ06Nfbt22dGIYPy8/Nj165d\nsXfv3hg5cmR89tlnsW/fvnj66adj7969sWfPHvMJGZSTkxNbtmyJkpKSKCgoiB07dsQPP/wQjz32\nWPzxxx+uoZAhu3btioaGhrjzzju7bUQ91Y9yOjs7OxP9LP8T6uvrY82aNVFTUxMjR46MBQsW+F8H\nyJCmpqZ44oknLlhfu3ZtRIRZhSzz5ptvxtChQ2PGjBmup5AFGhoaYt26dbF3794oKio6/43j5hOy\nw1dffRWbNm2KEydOxLBhw+LBBx+McePGmVHIoM2bN0dNTU1UVlZGRPeNqCdmtddHOAAAAABIrVd/\nJhwAAAAA/DeIcAAAAACQmAgHAAAAAImJcAAAAACQmAgHAAAAAImJcAAAAACQmAgHAAAAAImJcAAA\nAACQmAgHAAAAAImJcAAAAACQ2P8B4GU2xbENR9EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7168d3b390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Score: 0.123\n",
      "Confusion Matrix: \n",
      "[[  0   0   0   0   0   0   0   0 299   0]\n",
      " [  0   0   0   0   0   0   0   0  66   0]\n",
      " [  0   0   0   0   1   1   0   0 292   0]\n",
      " [  0   0   0   0   2   1   0   0 207   0]\n",
      " [  0   0   0   0   6  11   0   0 232   0]\n",
      " [  0   0   0   0   0   0   0   0 292   0]\n",
      " [  0   0   0   0   0   0   0   0  10   0]\n",
      " [  0   0   0   0   0   0   0   0 253   0]\n",
      " [  0   0   0   0   0   0   0   0 270   0]\n",
      " [  0   0   0   0   0   0   0   0 300   0]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "training_epochs = 50\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "saver = tf.train.Saver()\n",
    "samples = np.arange(np.shape(train_x)[0])\n",
    "\n",
    "print \"Training.......\"\n",
    "print \"------ Elapsed time -------  Iter ---- Cost \"\n",
    "start_time = time.time()\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    session.run(init)\n",
    "    for epoch in range(training_epochs):    \n",
    "#         offset = (itr * batch_size) % (train_y.shape[0] - batch_size)\n",
    "#         batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "#         batch_y = train_y[offset:(offset + batch_size), :]\n",
    "        np.random.shuffle(samples)\n",
    "        batch = samples[0:batch_size]\n",
    "        samples = samples[batch_size:]\n",
    "        batch_x = train_x[batch]\n",
    "        batch_y = train_y[batch]\n",
    "        feed_dict={X: batch_x, Y : batch_y, keep_prob : 0.5}\n",
    "        _, c = session.run([optimizer, cost],feed_dict=feed_dict)\n",
    "        cost_history = np.append(cost_history,c)\n",
    "        if(itr % (training_iterations/10) == 0):\n",
    "            print \"------ {:12.7f} ------- {:5d} ---- {:12.10f} \".format((time.time() - start_time), itr, c)\n",
    "    \n",
    "    y_pred = session.run(tf.argmax(y_,1),feed_dict={X: test_x})\n",
    "    y_true = session.run(tf.argmax(test_y,1))\n",
    "    print (\"---Training time: %s seconds ---\" % (time.time() - start_time))\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    plt.plot(cost_history)\n",
    "    plt.axis([0,training_iterations,0,np.max(cost_history)])\n",
    "    plt.show()\n",
    "    p,r,f,s = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    save_path = saver.save(session,\"model/cnn.ckpt\")\n",
    "    print \"F-Score:\", round(f,3)\n",
    "    print \"Confusion Matrix: \"\n",
    "    print confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
